\documentclass{ar2rc}

\usepackage{siunitx}
\usepackage{apacite}
\usepackage{physics}

\usepackage[colorinlistoftodos]{todonotes}
\usepackage{color}
\newcommand{\jdg}[1]{\todo[color=red!30]{#1}}
\newcommand{\all}[1]{\todo[color=blue!30]{#1}}

% better lists
\usepackage{enumitem}
\setlist{nosep}

% some commands
\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}
\newcommand{\usd}[1]{\SI{#1}[\$]{}}

% ACRONYMS
\usepackage[acronym, nopostdot, nonumberlist, shortcuts, numberedsection, nogroupskip,]{glossaries}
\newacronym{bfe}{BFE}{base flood elevation}
\newacronym{cmip}{CMIP}{the Coupled Model Intercomparison Project}
\newacronym{dmdu}{DMDU}{decision making under deep uncertainty}
\newacronym{fema}{FEMA}{the Federal Emergency Management Agency}
\newacronym{gev}{GEV}{generalized extreme value}
\newacronym{hazus}{HAZUS}{Hazard U.S.}
\newacronym{iid}{IID}{independent and identically distributed}
\newacronym{ipcc}{IPCC}{International Panel on Climate Change}
\newacronym{msl}{MSL}{mean relative sea level}
\newacronym{noaa}{NOAA}{the National Oceanic and Atmospheric Administration}
\newacronym{pdf}{PDF}{probability density function}
\newacronym{rcp}{RCP}{representative concentration pathway}
\newacronym{rdm}{RDM}{robust decision making}
\newacronym{slr}{SLR}{sea level rise}
\newacronym{ssp}{SSP}{shared socio-economic pathway}
\newacronym[]{usace}{USACE}{United States Army Corps of Engineers}
\newacronym[]{usgs}{USGS}{United States Geological Survey}
\newacronym[\glslongpluralkey={states of the world}]{sow}{SOW}{state of the world}

% cross-refs
\usepackage{xr}
%\externaldocument{./submission_02}
\hypersetup{hidelinks}
\usepackage{cleveref}

\title{A subjective Bayesian framework for synthesizing deep uncertainties in climate risk management}
\author{James Doss-Gollin and Klaus Keller}
\journal{Earth's Future}

\begin{document}

\maketitle

We thank the editors and both referees for their thoughtful, detailed, and extremely constructive comments on our initial submission.

Referee 1 particularly encouraged us to clarify the presentation of our methods and results.
As noted in responses to the specific comments given below, we have attempted to incorporate these suggestions into the revised manuscript.

Referee 2 encouraged us to (1) better contextualize our methodological approach relative to previous works, (2) clarify and improve the mathematical formulation of our approach, and (3) to better describe the usefulness of our method.
The specific

In the remainder of this document we address the specific comments made by the two referees.

\section{Referee \#1}

\RC{
    This work introduces a Bayesian framework to synthesize and characterize deep uncertainties to merge standard exploratory modeling techniques with more traditional decision support tools that require statistics like the expected value.
    Such statistics, which require weighting sampled states of the world, are problematic in the presence of deep uncertainty.
    The authors argue that many decision making frameworks commonly used for robust decision-making problems actually make such a weighting, but that is not transparent.
    Decisions about the sampling range, joint sampling distribution, and statistics (such as domain criterion) can have consequences for the robustness of the options presented to stakeholders, but this may not be clear.
    The proposed framework allows for a synthesis across deep uncertainties using subjective Bayesian priors.
    The method is demonstrated for a very simple house-raising example including both sea level rise and a stationary storm surge risk.

    Overall, I found the work well written and informative, and I think it is very promising.
    The framework's novelty arises from the way it stitches standard methods from Bayesian statistics and DMDU.
    This is not a negative statement, as I think by employing common-sense methods in a novel way the authors' framework is more likely to gain traction.
    An important contribution is the introduction, which nicely lays out the current state of practice, emerging methods (primarily from the DMDU community), and then research gaps.
    I appreciated the author's care in discussing the difficulty in adopting non-stationary frequency methods for extremes, with key citations to important works by Montanari, Seranaldi, and Bulletin 17C's introduction.
}

\AR{
    Thank you for your comments.
    This is an excellent summary of the aim and scope of this paper.
}

\RC{
    The use of the word model to mean different things. Throughout the manuscript, and particularly in Figure 1, the authors utilize the term `model' to refer to both the combination of RCP-SLR model/assumption and the systems model.
    I found this confusing at multiple places.
    One suggestion is to call the 'models' in box d of Figure 1 'scenarios' or something like that.
    So the scenario is the deep uncertainty (RCP-SLR model) and then the possible futures in box b of Figure 1 are samples drawn from the pdf that's conditioned on the scenario in Box d.
    I think this new terminology would be particularly important if one were to have more than one systems model in box c.
}

\AR{
    Thank you for flagging this as an opportunity to improve clarity.
    After some discussion we have decided to refer to ``scenarios'' (where it adds clarity, ``probabilistic scenarios'') and ``models'' (or ``physical models'' where it is helpful to draw distinctions from, \eg, system models).
}

\RC{
    I understand the intention of the paper is to showcase the method, not necessarily the results.
    This said, some more discussion about the findings would be helpful.
    For example, in Figure 7A there is a spike after 0 ft of increased height. If some detail were given on the construction cost function (taken from another work), and at least a few more sentences on the results, I think it would really improve an already very good manuscript.
}

\AR{
    Good suggestion, can do.
}

\begin{quote}
    Copy and paste in the new text where we discuss figure 7A
\end{quote}

\RC{
    Line 270: I think more detail should be given, perhaps in the discussion, of the difficulty projecting SOW to some lower dimensional representation may present in more complicated problems.
    Here it is given little attention, which is fine because this is meant to be a simple didactic problem.
    But I could imagine a much more complicated setting, perhaps in a multisector problem, where collapsing the state space of the world presents problems.
    I suggest the authors provide some more discussion of the challenge this may present and how one might overcome it in the discussion.
}

\AR{
    Yes, we have a bit of discussion about this but more would be helpful.
}

\begin{quote}
    Copy and paste in the new text
\end{quote}

\RC{
    Line 284: One really exciting direction is the opportunity to test the sensitivity of the objective tradeoffs to $p_\textup{belief}$.
    It may be the case that some objectives are not affected by the differences in $p_\textup{belief}$ between stakeholders and so there is no point in getting tied up trying to pin it down.
    I'm not proposing any additional analysis, only that this sort of screening is possible and is consistent with the spirit of exploratory modeling and RDM.
    It would be an interesting exercise, perhaps for follow-on work.
}

\AR{
    Yes, good suggestion and we can elaborate on this ni the discussion.
}

\begin{quote}
    Copy and paste in the new text
\end{quote}

\RC{
    Table 1: I think at least some detail should be given about the elevation cost and depth-damage functions.
    I understand they were taken from another work. This would make interpreting the later results easier.
}

\AR{
    Yes, can do, especially since the shape of these curves matter a lot.
    For example, findings that elevating a little bit is ineffective in nearly all circumstances stem from the shape of the construction cost curve.
}

\begin{quote}
    Copy and paste in the new text
\end{quote}

\RC{
    Figure 5b.
    If the intention is to show the agreement of the fitted model with the annual maximum storm surges, a P-P plot might be clearer.
    I leave this up to the authors.
    I have no doubt the GEV is a good fit.
}

\AR{
    The purpose here is to show the probability distribution using a standard return period plot.
    But a Q-Q or P-P plot in the supplemental material to assess fit would be good.
}

\RC{
    Figure 4b.
    There is remarkable disagreement between the models of mean sea level rise.
    It might be worth at least a few sentences commenting on this.
}

\AR{
    We mainly defer to \citeA{ruckert_coastal:2019} on this, but we can add a bit of discussion.
}

\begin{quote}
    Copy and paste in the new text
\end{quote}

\RC{
    Equation 4: it might be helpful to define $p$ here.
}

\AR{
    Can and should do.
}

\begin{quote}
    Copy and paste in the new text
\end{quote}

\RC{
    Line 416: The use of the gamma distribution is quite sensible but is presented without justification.
    This might be worth just a short sentence on why it was chosen.
}

\AR{
    Good suggestion.
}

\begin{quote}
    Copy and paste in the new text
\end{quote}

\RC{
    Figure 8: The y-axis labels on B-D should be more descriptive.
}

\AR{
    Yes, can do.
    See new figure.
}

\section{Referee \#2}

\RC{
    This manuscript addresses an important topic: the role of probabilistic methods in decision-making under deep uncertainty.
    The authors point out that scenario-based DMDU methods implicitly put probabilities on scenarios but do not make these assumptions transparent.
    They address this by developing a method to calculate the weights implicitly applied to each deeply uncertain scenario.
    I applaud this effort -- the tension between scenario-based methods that use optimization and probabilistic methods is an important and understudied one.
    I also see a lot of value in how the authors have framed the problem and described the pros and cons of scenario-based DMDU methods and probabilistic methods for CBA in the context of infrastructure design.
    The simple case study of raising the elevation of a house under flood risk affected by deeply uncertain sea level rise is appropriate for demonstrating the method and is technically sound.

    However, I have three major concerns that prevent me from recommending the manuscript for publication in its current form.
    I defer to the editor on whether, assuming all these concerns are addressed in revision, the contribution warrants publication in Earth's Future.
}

\AR{
    Thanks for the insightful comments.
}

\RC{
    First, the paper is not well grounded in previous literature at the intersection of Bayesian /probabilistic methods and DMDU.
    This has led the authors to make overly broad claims about their contribution that make it difficult to discern what they are adding beyond existing methods.
    It is certainly not true that ``In this paper we offer a first conceptual step towards bridging the fields of DMDU and Bayesian model building'' as claimed in line 183.
    Here are some papers that integrate Bayesian/probabilistic methods into DMDU contexts:
    \citeA{reis_distribution:2020},
    \citeA{taner_probabilistic:2019},
    \citeA{hui_adaptive:2018},
    \citeA{fletcher_learning:2019}, and
    \citeA{fletcher_groundwater:2019}.
    The first three are especially similar to the authors' contributions; the authors need to make a compelling argument for what their approach adds beyond this existing literature.
}

\AR{
    Thanks very much for this helpful comment.
    This comment underscores a need for improved clarity in the introduction to the manuscript; as noted, there are numerous papers that use probabilistic tools, including Bayesian methods, in DMDU contexts.
    We have clarified that there is a long literature on the relative merits of probabilistic vs non-probabilistic information in DMDU contexts, as noted in \citeA{taner_probabilistic:2019}, but that this debate has often centered on a top-down, technocratic, objectivist philosophy of probability in which there is explicitly or implicitly assumed to be a true probability distribution.
    Our contribution is to outline how a subjective Bayesian philosophical approach can address many of the concerns raised by critics of probabilistic approaches while providing a principled and transparent method for reasoning about uncertainty.

    We can add a paragraph to the introduction describing this intersection, possibly with a few additional papers.
    \citeA{hui_adaptive:2018} is a nice example of the use of SDP applied to a levee heightening problem.
    There is Bayesian updating through the dynamic programming, but deep uncertainty is addressed only by considering several scenarios of hydrological conditions.
    \citeA{taner_probabilistic:2019} is conceptually quite similar to our approach.
    This paper focuses on joint distributions of uncertain parameters rather than how different subjective beliefs about these parameters affect the decision.
    This paper also focuses on how probabilistic versus exploratory frameworks lead to different robustness values, whereas here we focus more generally on mapping trade-offs.
    \citeA{reis_distribution:2020} shows that scenario discovery and exploratory modeling results are sensitive to parameterization and choice of distribution.
    \citeA{fletcher_learning:2019} and \citeA{fletcher_groundwater:2019} apply SDP (which can be interpreted as having an implicit Bayesian updating step) but shallow uncertainties are considered.
}

\begin{quote}
    Add a sentence to the intro and link it here
\end{quote}


\RC{
    My second key concern is that the mathematical formulation is not clear and complete, and it is not always clear how the sample application aligns with the general framework.
    Given that the authors suggest the mathematical formulation is a key part of their contribution, this is important.
    Specifically:
    The method is framed both in the introduction (lines 157-182) and methods section (lines 240-250) around Bayesian model selection.
    However, it is not clear if or how the authors are performing Bayesian model selection.
    In figure 1e and line 247, the authors provide a conditional probability of $u$ given $x_i$ and $M_k$, and state this is the result of IID draws from $M_k$.
    This is similar to the $p(y|M_k)$ term in the classical BMA framework from \citeA{raftery_bma:2005} equation 1.
    Does this imply that there is a Bayesian posterior for model weights developed?
    If so, what is it?
    What data is used for the updating and what is the likelihood function? It is unclear if or how Bayes' is used here.
}

\AR{
    This is a great question and one we need to clarify.
    For reference, equation 1 of \citeA{raftery_bma:2005} is
    $$p(y) = \frac{1}{K} \sum_{k=1}^K p(y|M_k) p(M_k | y^T),$$
    ``where $p( y | M_k)$ is the forecast PDF based on model $M_k$ alone, and $p(M_k | y^T)$ is the posterior probability of model $M_k$ being correct given the training data, and reflects how well model $M_k$ fits the training data.''

    This comment is closely related to the previous comment in that we draw inspiration from the literature on Bayesian model selection but do not explicitly perform it.
    Our approach is conceptually similar to BMA as noted (and even close to stacking; see \cite{Yao:2018bu}).
    There are two key differences.
    First, these methods focus on estimating the weights assigned to each model from data; in our context we have no data.
    Future work could consider adaptive decision making (by revisiting the decision at multiple time steps, \ie stochastic control) in which case there would be data available in the future that could be used to estimate these weights as in several of the papers referenced in the previous comment.
    Second, these methods assign weights to each model, whereas we assign weights to each SOW.
    This is motivated by the fact that the SOWs in our applications, and in many climate adaptation applications, are computationally expensive to generate.
}

\begin{quote}
    Need to think about how we want to address this in the text
\end{quote}


\RC{
    Shortly later in the paragraph under section 2.3, the authors take an expected value of $f(s)$ and state that it ``can be readily estimated as $\frac{1}{N} \sum_{j=1}^N f(\bf{s}_j)$.''
    I believe this is inconsistent with the interpretation of the SOW being samples from an underlying distribution.
    We know that:
    $$\mathbb{E}\qty[f(s)]=\sum_{s \in \Omega}f(s)P_s(s)$$
    where $P_s$ is the probability mass function of $s$.
    This is equivalent to the authors' equation above only when assuming that $P_s(s)$ is a discrete uniform RV where each SOW gets equal probability.
    However, this seems to be at odds with the authors interpretation that the SOW are IID draws from some underlying distribution of models.
    If that is the case, the probability mass or density function shouldn't simply put probability on the available SOW, as these are merely samples from an underlying distribution.
    It conflates the sample with the population.
    If, for example, the SOW were believed to be IID samples from a lognormal distribution, the we would expect an estimate for $\mathbb{E}\qty[f(s)]$ that is asymmetric across the $f(s)$ in different SOW, not an unweighted average.
}

\AR{
    This is another insightful comment, and I think we can frame this a bit better in the literature on non-naive Monte Carlo sampling.
    As noted, we write in section 2.3 that
}
\begin{quote}
    If the \glspl{sow} are drawn \gls{iid} from the true distribution, then the expected value of some function  $f$, $\mathbb{E}\qty[f(\vb{s})]$, can be readily estimated as $\frac{1}{N} \sum_{j=1}^N f(\vb{s}_j)$.
    However, this is often not the case; for example, a fixed set of simulations may be available for analysis or low-probability but high-impact regions of the parameter space may have been sampled.
    In this case, the formula must be adjusted to
    \begin{equation*}
        \mathbb{E}\qty[f(\vb{s})] \approx \frac{1}{N} \sum_{i=j}^N w_j f(\vb{s}_j),
    \end{equation*}
    where the $w_j$ are suitably chosen weights such that their sum is equal to 1.
\end{quote}

\AR{
    This does not require assuming that the set of sows, $\vb{s}$, is a discrete uniform RV, only that they are drawn IID from $p(\vb{s})$
}

\begin{quote}
    Need to revisit -- the comment about population doesn't make sense to me
\end{quote}

\RC{
    I am further confused by equation 1 and the following paragraph on choosing the weights.
    First, are the weights “chosen” by the analyst, as stated in line 266, or are they estimated as a way of making clear what weights are implied by a certain selection of SOW?
}

\AR{
    Let's make sure it is clear that they are implied by a certain $p_\text{belief}$ and sampling distribution of SOWs
}

\begin{quote}
    Let's change it so that instead of saying ``suitably chosen'' weights we just say $\text{s.t.}~ \sum_j w_j = 1$.
    There's another set of questions.
\end{quote}

\RC{
    Second, what is the mathematical basis for equation 2?
    The description in 267- 272 describes the method illustrated in figure 2 but doesn't justify it, and I could not find the method in the references provided.
    It seems like the approach is a way of approximating some unknown continuous distribution of $M_k$ using the discrete sampled SOW, but the authors do not state this.
    If this is true, it could potentially be used to address my comment above about conflating sample and population.
}

\AR{
    This is just a definition, but we can cite something like \citeA{johnson_clara:2013} that uses grid-based methods
}

\RC{
    Finally, I am confused how the application in section 4.3 is equivalent to this.
    Line 413 states ``we construct three probabilistic models for $p_\textup{belief}$'' and then refers to these models in line 418 as ``priors''.
    The use of the word “prior” implies a Bayesian model.
    Does this suggest that equation (2) is meant to be a prior distribution? It is not described that way in section 2.3. If so, what is the posterior? What is the likelihood? My interpretation is that the “target distribution” in figure 2 is F belief, but that the weights over SOW are just an approximation of that, not a posterior.
}

\AR{
    Another good point -- need to clarify that refer to priors because we don't have any data about the future so we are relying entirely on prior beliefs.
    But we're not applying Bayes' rule.
}

\RC{
    It is clear the storm surge model in 3.2 uses a Bayesian model to fit a GEV using historical storm surge data. But this is presented as a probabilistic uncertainty, not a deep uncertainty, so it remains unclear how Bayesian methods are used to synthesize deep uncertainties.
}

\AR{
    We can discuss this point.
}

\RC{
    It is not clear to me if there is an error in the mathematical presentation or if it simply needs to be clarified. It is possible also that this is a valid probabilistic framework, but not a Bayesian one, in which case the approach would need to be substantially reframed. If I have misunderstood due to lack of clarity in the presentation, I welcome a clarification from the authors.
}

\AR{
    I think we can clarify and need to.
    In part, at least, the term ``Bayesian'' seems to be suggesting a method of estimation, which is not what we are doing.
}

\RC{
    My final key critique is that I think the authors need to make a more compelling case for the usefulness of their approach relative to existing DMDU methods to warrant
    publication. If I am interpreting correctly, the two new aspects of the method are 1) the conditioning step (section 2.2 which I believe though am not sure is the same as what is applied in section 4.2) and 2) the deep uncertainty synthesis. The key result from the conditioning step is show in Figures 6 and 7. It seems to me that Figure 6 is quite similar to visuals commonly used in exploratory analysis like parallel axis plots or scatter plots showing the results against different objectives across SOWs. Take as an example Figure 6 from Quinn et al. 2020. This plot shows frequency and severity of water shortage as measured by cumulative frequency across SOWs in the experiment, visualized with darker colors for more SOWs. This is a common type of figure in exploratory analysis used to transparently visualize the results across SOW. The authors figure 6 seems quite similar to this. What is more useful or transparent about your conditioning approach? I mean that as a genuine question – I have found the methods difficult to follow and would appreciate clarification from the authors.
}

\AR{
    Thanks, we can add a bit to the discussion.
}

\RC{
    Similarly, Figure 8 shows the key results from the weighting step.
    As I understand it, panel a shows the authors illustrative assumptions for sea level rise distributions, and then panels b-d show what weights across SOWs are consistent with those assumptions. What should a decision maker takeaway from this? How might they use it in their decision process? It would be helpful to see this articulated in the example and expanded on in the discussion or conclusions.
}

\AR{
    This should come across more clearly in the conclusions.
}

% ugly hack to hide month in APACite
\renewcommand{\APACrefYearMonthDay}[3]{\APACrefYear{#1}}
\bibliographystyle{apacite}
\bibliography{library-bibtex}

\end{document}
