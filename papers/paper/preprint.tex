\documentclass[11pt]{article}

% Figures
\usepackage{graphicx}
\usepackage[list=true]{subcaption}
\graphicspath{{../../plots/}{../../tikz/}{../../img/}}
\usepackage[section]{placeins} % require floats to appear in the section they are defined

% fonts and appearance
\usepackage{amsmath, amsfonts, physics, siunitx, nicefrac}
\usepackage[american]{babel} 
\usepackage[T1]{fontenc} % improved font encoding
\usepackage[ttscale=0.8]{libertine}
\usepackage{fontawesome5}
\usepackage[format=plain, textfont=it]{caption}

% page size and margins
\usepackage{geometry}
\geometry{letterpaper,top=1in, bottom=1in, left=1in, right=2in}

% footer
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[en-US]{datetime2}
\fancyhf{}
\fancyhead[L]{CONFIDENTIAL DRAFT by J. Doss-Gollin \& K. Keller}
\fancyhead[R]{\DTMnow}
\fancyfoot[R]{page~\thepage~of~\pageref{LastPage}}
\pagestyle{fancy}
\setlength{\headheight}{15pt}

% TO DO NOTES
\usepackage{xcolor} % list of colors at https://en.wikibooks.org/wiki/LaTeX/Colors
\definecolor{giallo}{HTML}{F0BC42} % https://teamcolorcodes.com/a-s-roma-color-codes/
\definecolor{rosso}{HTML}{8E1F2F}
\definecolor{grigio}{HTML}{CACACC}
\definecolor{nero}{HTML}{000000}
\usepackage[textsize=scriptsize]{todonotes}
\setlength{\marginparwidth}{1.5in}
\newcommand{\james}[1]{\todo[color=giallo, textcolor=nero]{\textbf{ATTN James:~}#1}} % if desired create a custom command for each author
\newcommand{\klaus}[1]{\todo[color=rosso, textcolor=grigio]{\textbf{ATTN Klaus:~}#1}}

% better tables
\usepackage{booktabs}
\usepackage{array}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

% better lists
\usepackage[inline]{enumitem}
\setlist{nosep}

% authors
\usepackage{authblk}
\title{A subjective Bayesian framework for synthesizing deep uncertainties in climate risk management}
\author[1]{James Doss-Gollin}
\author[2]{Klaus Keller}
\affil[1]{Department of Civil and Environmental Engineering, Rice University}
\affil[2]{Thayer School of Engineering, Dartmouth College}
\renewcommand*{\Affilfont}{\normalsize\normalfont}

% ACRONYMS
\usepackage[acronym, nopostdot, nonumberlist, shortcuts, numberedsection, nogroupskip,]{glossaries}
\newacronym{bfe}{BFE}{base flood elevation}
\newacronym{cmip}{CMIP}{the Coupled Model Intercomparison Project}
\newacronym{dmdu}{DMDU}{decision making under deep uncertainty}
\newacronym{fema}{FEMA}{the Federal Emergency Management Agency}
\newacronym{gev}{GEV}{generalized extreme value}
\newacronym{hazus}{HAZUS}{Hazard U.S.}
\newacronym{iid}{IID}{independent and identically distributed}
\newacronym{ipcc}{IPCC}{International Panel on Climate Change}
\newacronym{msl}{MSL}{mean relative sea level}
\newacronym{noaa}{NOAA}{the National Oceanic and Atmospheric Administration}
\newacronym{pdf}{PDF}{probability density function}
\newacronym{rcp}{RCP}{representative concentration pathway}
\newacronym{rdm}{RDM}{robust decision making}
\newacronym{slr}{SLR}{sea level rise}
\newacronym{ssp}{SSP}{shared socio-economic pathway}
\newacronym[]{usace}{USACE}{United States Army Corps of Engineers}
\newacronym[]{usgs}{USGS}{United States Geological Survey}
\newacronym[plural=SOWs,firstplural=states of the world (SOWs),descriptionplural=states of the world]{sow}{SOW}{state of the world}
\makeglossaries % required to make the list of acronyms

\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}

\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\newcommand{\usd}[1]{\SI{#1}[\$]{}}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}

% use biblatex
\usepackage{csquotes}
\usepackage[
  backend=biber,
  doi=true,
  url=false,
  isbn=false,
  style=authoryear-comp,
  natbib=true,
  backref=false,
  maxbibnames=10,
  maxcitenames=2,
  uniquename=false,
  uniquelist=false,
  sorting=nyt,
  giveninits=true,
]{biblatex}
\renewbibmacro{in:}{}
\AtEveryBibitem{\clearfield{month}\clearfield{day}\clearfield{pages}\clearlist{language}}
\addbibresource{library.bib}

% load this last
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}

\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}
    Current practice in engineering, infrastructure design, and local governance relies heavily upon standards that specify particular design events or conditions that buildings and infrastructure should safely withstand.
    The
\end{abstract}

Key points
\begin{enumerate}
    \item
    \item
    \item
\end{enumerate}

\clearpage
\section{Introduction}\label{sec:introduction}

Many critical infrastructure services around the world are aging and inadequate for a changing climate \citep[\eg,][]{doss-gollin_txtreme:2021,doss-gollin_fatalism:2020,chester_reliable:2020}.
Looking to the future, it is expected that changes to regulation, economics and finance, patterns of population and infrastructure use, as well as climate, will further stress infrastructure systems.
This motivates the question: which possible futures should infrastructure systems and components be designed for?
The answer to this question depends in part on values: intrinsic trade-offs between safety, performance, cost, and other objectives depend on context and stakeholder preferences \citep{keller_management:2021} and are often regulated by statue or industry guidelines.
For example, hospitals and critical infrastructure are generally designed to a higher standard of risk protection than ordinary buildings.
At the same time, performance depends upon future conditions, and so decisions about which scenario to design for are necessarily subject to implicit or explicit assumptions about the likelihood or possibility of different future conditions.

\subsection{Current practice}

Current practice in engineering, infrastructure design, and local governance relies heavily upon standards that specify particular design events or conditions that buildings and infrastructure should safely withstand \citep{asce_7-10:2013,bruneau_multihazard:2017}.
These are often, though not always, informed by probabilistic analysis of relevant data.
For example, \gls{fema}, local governments, and engineering consultants produce local floodplain maps in many communities.
These trigger specific floodplain regulations, such as a requirement for homes in the 100-year floodplain with federally backed mortgages to be covered by flood insurance \citep{kousky_voucher:2014}.
Additionally, local building codes \citep[based on guidance such as][]{asce_24-05:2006,FEMA_p-55:2011} may require new construction in flood zones to be elevated freeboard above a nominal \gls{bfe}.
Design standards are also used to design large-scale infrastructure.
For example, some levees in the Netherlands are required to be designed such that the annual probability of failure is less than 1 in \num{4000} \citep{eijgenraam_flooding:2014}, while a seawall proposed as part of a \$29 billion coastal protection project for  Galveston Bay was designed by setting the nominal annual probability of overtopping to 1\% \citep[Appendix D.,~p.~2-59]{USACE_coastal:2021}.

There are many advantages to standards-based design.
In particular, these heuristics are scalable and explainable, do not require complex analysis to apply, and are fair in at least a procedural sense.
However, there are also limitations.
In particular, this sort of one-size-fits-all guidance may not be an efficient or desirable way to balance tradeoffs.
This has motivated economically informed approaches, like risk-based design and cost benefit analysis \citep{eijgenraam_flooding:2014,vandantzig_dike:1956,xian_elevation:2017}, that place ``a strong emphasis upon a proportionate response to risk, so that the amount invested in risk reduction is in proportion to the magnitude of the risk and the cost-effectiveness with which that risk may be reduced'' \citep{merz_fluvial:2010}.
These quantitative cost-benefit analyses also rely on probabilistic descriptions of relevant hazard, and may be particularly sensitive to representation of tail probabilities.

Currently used methods for estimating these \glspl{pdf} emphasize data-driven, nominally objective methods that can be applied consistently across locations.
For example, \gls{usgs} Bulletin 17C specifies procedures for estimating flood frequency and \gls{noaa} Atlas 14 provides estimates of the intensity, duration, and frequency of extreme rainfall.
Among several statistical assumptions these analyses make that have been recently called into question is stationarity \citep[the assumption that past and future hazard come from the same \gls{pdf}; see][for a review]{merz_review:2014}.
For example, clear trends in extreme rainfall are apparent across much of southeastern Texas \citep{fagnant_spatiotemporal:2020,nielsen-gammon_txrainfall:2020}, and trends in many other hazards are consistent with observations \citep[see][for a comprehensive summary]{ipcc_impacts:2022}.\klaus{I'm citing the technical summary which is 96 pages -- there are about 100 authors listed. Any suggestion for how to cite best?}
While some methods have been proposed for incorporating trends into these analyses \citep[see][for a review]{Salas:2018ge}, these assume specific forms of nonstationarity which may not fully represent physical processes or true levels of uncertainty \citep{DossGollin:2019,Montanari:2014hl,Serinaldi:2015bq}.
Ultimately, the difficulty of incorporating nonstationarity into existing frameworks has led to continued reliance on stationarity despite recognition of the associated limitations (see \cite{bulletin17c:2019}, p.~2, or \cite{atlas14_texas:2018} p.~A.4-42).

\subsection{Emerging paradigms}

Projecting nonstationary hazard is difficult because many future hazards depend on intrinsically unpredictable human decisions (\eg, the rate of future greenhouse gas emissions) or on physical processes that are poorly constrained by existing data \citep[\eg, collapse of the West Antarctic ice sheet; see][]{deconto_antarctica:2016}.
In other words, they are deeply uncertain \citep{keller_management:2021,walker_deep:2013,lempert_complex:2002}.
These deeply uncertain nonstationary hazards challenge not only the existing stationary estimates but, more fundamentally, the premise that objective estimates of future hazard exist and can be estimated empirically.

Recognizing the challenges of \gls{dmdu}, many frameworks for identifying robust decisions have been proposed.
Most emphasize the use of models in an exploratory (``what-if'') framework to learn about interactions between decisions and system dynamics \citep{bankes:1993}.
For example, robust decision making \citep{lempert_shaping:2003} evaluates models over large ensembles of possible futures to assess the performance of different policies under each, then applies statistical analysis to identify the conditions under which particular policies perform well or poorly.
When the decision space is complex, many analyses use policy search (often using multiobjective optimization tools) to identify promising actions \citep{kasprzyk:2013,kasprzyk_denovo:2012,hadka_mordm:2015}.
In addition, many studies formally quantify robustness to deep uncertainties \citep{herman:2015,mcphail_robustness:2019} and use this as a criterion for policy comparison.

Although these methods for \gls{dmdu} have proven valuable in a wide range of settings, they still require necessarily subjective assessments about the likelihood of future conditions.
Even exploratory models require the analyst to choose which uncertainties are considered and how they are sampled.
When analyses use metrics that integrate performance over many possible futures, whether to compute sensitivities, estimate expectations, or compute robustness metrics, they necessarily make assumptions about the likelihood of different futures.
For example, uniform priors are often justified by deferring to Laplace's principle of insufficient reason \citep[see][p.~135]{stigler_uncertainty:1986}, but even this choice makes strong assumptions that are sensitive to factors such as parameterization \citep[p.54]{gelman_bda3:2014}.
While one interpretation of deep uncertainty is that that specifying a joint \gls{pdf} over inputs is inappropriate, assumptions about the ranges and independence of parameters to sample are just as subjective as the choice of probability distribution \citep{schneider_scenarios:2002,quinn_exploratory:2020} and, indeed, can be interpreted  as a specific choice of probability distribution (we revisit this point in \cref{sec:analysis-condition}).
This motivates the development of decision analytic frameworks that draw from the strengths of \gls{dmdu} methods such as exploratory modeling, vulnerability assessment, robustness checks, and iterative stakeholder critique, but that embrace the reality that assumptions about the future are inescapable.

\subsection{Research gaps and objectives}

We are motivated by parallels between decision making under deep uncertainty and the statistical problem of model selection.
\citet{oreskes_verification:1994} argues that because natural systems are never closed and model results are never unique, validation and verification of models representing these systems is necessarily qualitative and subjective.
In the literature on decision making under deep uncertainty, this has led to recognition of the need to develop strategies that are robust to model imperfections.
We find parallels to this challenge in the Bayesian literature on model selection, particularly in the ``$\mathcal{M}$-open'' case in which there is no ``true'' model to identify and all models are imperfect representations of reality.
This is a theoretical contrast to the ``$\mathcal{M}$''-closed case in which a true model can be identified and written down but is one amongst finitely many models from which an analyst has to choose as well as the ``$\mathcal{M}$''-complete case in which a true model exists but is inaccessible in the sense that even though it can be conceptualized it cannot be written down \citep{bernardo_bayesian:1994}.
Whereas model selection in the $\mathcal{M}$-closed case seeks to asymptotically identify the true model from a set of candidate models, model selection in the $\mathcal{M}$-open case emphasizes a subjective view, both of probability itself and of the modeling process, for which probability offers a self-consistent language with which to reason about the unknown rather than a statement of objective truth (see \cite{gelman_philosophy:2013}, \cite{ramsey_probability:2016}, or \cite{jaynes_probability:2003} for a discussion of Bayesian philosophy and \cite{Piironen:2017eh} for a technical discussion of methods for model selection).
From a practical perspective, model selection in the $\mathcal{M}$-open case emphasizes building models iteratively, simulating the consequences of these models, and subsequently using these simulations to critique and improve them \citep{gelman_workflow:2020}.

In this paper we offer a first conceptual step towards bridging the fields of \gls{dmdu} and Bayesian model building.
We consider a didactic case study of whether to elevate a hypothetical house, and if so how high, as a specific example of a decision problem subject to both shallow (storm surge) and deep (\gls{slr}) uncertainties.
Prior studies have found that floodproofing and building-scale vulnerability reduction measures, including house elevation, can effectively reduce local flood damages in many contexts \citep{demoel_reducing:2014,deruig_building:2020,kreibich_building:2005,slotter_floodproofing:2020,rozer_coping:2016,mobley_mitigation:2020,aerts_cost:2018}, and both local building codes \citep{asce_7-10:2013,bruneau_multihazard:2017,asce_24-05:2006} and federal policy \citep{FEMA_p-55:2011} require elevation in some cases.
Guidance for homeowners, notably from \gls{fema}, recommends elevating to the \gls{bfe} (typically the \SI{100}{year} flood) plus a freeboard \citep{fema_retrofitting:2014,asce_24-14:2015,fema_retrofitting:2014} but recent research has demonstrated that neglecting uncertainty in the cost-benefit analysis can lead to poor decisions \citep{zarekarizi_suboptimal:2020}.
Focusing on deep uncertainty in \gls{slr} over the 70\james{Need to re-run codes with 70 rather than 71} year design life of a hypothetical house, we seek to answer the research question ``\emph{how can deep uncertainties be transparently synthesized for decision analysis?}''

We proceed as follows.
In \cref{sec:analysis} we present three formal decision analytic frameworks for analyzing an ensemble of \gls{slr} simulations, building through existing approaches for exploratory modeling (\cref{sec:analysis-explore}) and scenario analysis (\cref{sec:analysis-condition}) towards a formal Bayesian method for transparently synthesizing deep uncertainty through subjective prior beliefs.
In \cref{sec:case-study} we describe the case study in detail.
Next in \cref{sec:results} we present results for each of the three decision lenses and discuss the advantages and limitations of each theoretical approach.
Finally in \cref{sec:conclusions} we discuss key findings, future research needs, and implications for research, policy, and practice.

\section{Decision analytic framework}\label{sec:analysis}

In this section we outline our framework for decision analysis under deep uncertainty, maintaining a high level of generality.
In the next we discuss application to the house elevation case study.

Following \cref{fig:flowchart}, we consider using $J$ \glspl{sow}, $\vb{s} = \qty{s_1, s_2, \ldots, s_J}$, to evaluate $I$ candidate decisions, $\vb{x} = \qty{x_1, x_2, \ldots, x_I}$.
For each scenario $s_j \in \vb{s}$ and decision $x_i \in \vb{x}$ we use a system model $f$ (comprised of multiple components) to calculate a set of metrics describing the performance of decision $x_i$ on \gls{sow} $s_j$, which we denote $u_{ij} = f(x_i, s_j)$.
While we assume for simplicity that the decision space is known and finite, this approach could be coupled to a policy search model that proposes candidate decisions.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{bayes-rdm.pdf}
    \caption{
        Outline of the proposed decision-analytic framework.
        In \cref{sec:analysis-explore} we use an exploratory framework to quantify the performance (c) of candidate decisions (a) under a large ensemble of possible futures (b).
        In \cref{sec:analysis-condition} we illustrate the ``multiple PDF problem'' by creating probability distributions over outcomes (e) that are conditional upon specific models describing the likelihood of different futures (d).
        In our case study, these are \gls{rcp} scenarios and physical models of sea level rise.
        Finally in \cref{sec:analysis-synthesize} we propose a subjective Bayesian framework for synthesizing across deep uncertainties by re-weighting sampled futures (f).
    }\label{fig:flowchart}
\end{figure}

\subsection{Explore}\label{sec:analysis-explore}

A first analytical step is to use the model in an ``exploratory'' mode.
Exploratory modeling is averse to making explicit assumptions about the likelihood of different \glspl{sow} and instead seek to generate new knowledge \citep{bankes:1993} by systematically exploring a large number of possible futures, emphasizing interactions between different uncertainties \citep{reed_msdbook:2022}.
Exploratory modeling is often paired with analyses that identify relevant scenarios  \citep{lamontagne_discovery:2018,groves_scenarios:2007} or summarize a system's response to forcing \citep{Poff:2015jn,Steinschneider:2015kk,sriver_sealevel:2018}.
Despite the aversion to strong assumptions about the likelihood of different futures, subjective modeling decisions such as the choice of system model, the set of candidate decisions, the criteria used to assess outcomes, and the choice of how to sample \glspl{sow} strongly influence results \citep{quinn_exploratory:2020}.

\subsection{Condition}\label{sec:analysis-condition}

Although exploratory modeling is a useful framework for understanding systems, there are many questions that it cannot answer.
For example, answering questions like ``what is the 95th percentile of metric $u$ under decision $x$'' or ``which decision minimizes expected damages'' or ``what is the probability of exceeding a critical threshold'' requires constructing estimators for the probability distribution over outcomes \citep{schneider_scenarios:2002}.

One way to interpret an ensemble of \glspl{sow} is as \gls{iid} draws from some true data generating process.
We call this scenario-conditional probabilistic analysis, which commonly arises when a probabilistically calibrated model for relevant outputs (\eg, a physical model for \gls{msl}; see \cref{sec:case-slr}) is forced by a deterministic scenario (\eg, a \gls{rcp} scenario).
In this case, \emph{the set of available \glspl{sow} is assumed to come from the true (often called population) distribution, and thus the sample statistics are assumed to reflect the true values.}
We add this concept to our framework with boxes (d) and (e) of \cref{fig:flowchart}, denoting the particular scenario (\ie, the assumed input) $M_k$.
If the \glspl{sow} are drawn \gls{iid} from $M_k$, the set of outcomes $u_{i, j}$ can be interpreted as \gls{iid} draws from the conditional distribution over outcomes, $p(u | x_i, M_k)$, allowing estimation of descriptive metrics.
This approach allows for quantification of uncertainty \emph{within models}, but can only qualitatively characterize uncertainty \emph{between models} \citep{wong_nola:2017,ruckert_coastal:2019,sharma_rcp:2021}. % Vivek raises an interesting point -- what if we incorporate the choice of model into the data generating process and use a posterior probability of each model being true -- but I think that's basically what we are calling step #3

We can also use this approach to understand \gls{dmdu} methods that sample a set of parameters from fixed ranges.
For example, \citet{sriver_sealevel:2018} sample parameters describing the rate of \gls{slr} across a range of values to inform coastal adaptation.
Similarly, \citet{trindade_waterpathways:2020} checks the performance of candidate decisions against an ensemble of synthetic time series of streamflow, water demand, and other parameters by sampling parameters that transform the available data over a plausible range \citep[\ie, robustness metrics; see][for details]{mcphail_robustness:2019,herman:2015}.
Since sampling over a range is equivalent to sampling from a Uniform distribution, this assumption is equivalent to assuming the true inputs to come from independent Uniform distributions (one for each parameter) bounded by the plausible range.

Our primary concern is not that subjective assumptions about the likelihood of different futures are wrong -- this is, almost surely, inevitable -- but that when these assumptions are opaque and presented without critique or validation they may lead to poor decisions.
\james{Build this out -- it's not so much the potential for poor decisions so much as that it can be hard to undwind the decision and understand what's driving it. We cant know if critical assumptions turned out to be poor if we don't know what they are or how changing them would result in a different analysis. That said, I think they can also lead to poor decisions!}

\subsection{Synthesize}\label{sec:analysis-synthesize}

If the \gls{sow} ensemble is not drawn \gls{iid} from the true distribution (\eg, if one deliberately over-samples low-probability but high-impact \glspl{sow}) or if one wishes to synthesize \emph{across} multiple scenarios $M_1, M_2, \ldots, M_K$, then additional measures must be taken.
One possibility is to discard the \glspl{sow} ($\vb{s}$) and resample from some ``true'' distribution, which we shall denote $p_\mathrm{belief}$.
In practice, this is impractical because simulating $\vb{s}$ may rely on complicated models that are computationally expensive to re-run.

Instead of attempting to re-sample $\vb{s}$, we instead consider re-weighting.
A simple approach would be to assign each \gls{sow} $s_j$ a weight $w_j$ equivalent to the ratio of the desired (often called target distribution in survey analysis) distribution, $p_\mathrm{belief}$ and the sampling distribution, which could be approximated through methods such as kernel density estimation.
Paralleling joint probability methods for statistical analysis of tropical cyclones, we use a grid-based approach \citep{johnson_clara:2013,resio_probabilities:2007,toro_jpm-os:2010}.
First, we project the \glspl{sow} $\vb{s} \in \Omega$ onto a low-dimensional representation, which we denote $\qty{\psi_1, \psi_2, \ldots, \psi_J} \in \Psi$.
This allows us to specify a prior belief over this reduced space $p_\mathrm{belief}(\Psi)$ instead of over the full space $p_\mathrm{belief}(\Omega)$.
We then calculate a probabilistic weight $w_j \in \qty[0, 1]$ for each \gls{sow} $s_j$ so that the weighted distribution of $\psi_j$ closely approximates $p_\mathrm{belief}(\Psi)$.

We present here the case where the $\psi_j$ are one-dimensional.
We first sort the $\psi_j$  from least to greatest so that $\psi_{j-1} \leq \psi_j$, ($j \neq 1$).
Defining $F_\mathrm{belief}(s)$ to be the cumulative distribution corresponding to $p_\mathrm{belief}$, we calculate weights as
\begin{equation}\label{eq:weights}
    w_j = \begin{cases}
        F_\mathrm{belief}\qty(\frac{1}{2}\qty[\psi_1 + \psi_2])                                                                     & j = 1     \\
        F_\mathrm{belief}\qty(\frac{1}{2}\qty[\psi_{j} + \psi_{j+1}]) - F_\mathrm{belief}\qty(\frac{1}{2}\qty[\psi_{j-1} + \psi_j]) & 1 < j < J \\
        1 - F_\mathrm{belief}\qty(\frac{1}{2}\qty[\psi_{J-1} + \psi_J])                                                             & j = J.
    \end{cases}
\end{equation}
This step is illustrated in \cref{fig:grid-sketch}.
For higher dimensional projections, this equation can be extended by partitioning the parameter space, then integrating the \gls{pdf} $p_\mathrm{belief}$ over each region.
From a practical perspective, many simulation models round outputs, meaning that there is a possibility of having $\psi_j$ that are equal; in this case it may be helpful to add a small amount of noise to the $\psi_j$ before computing the weights.
Diagnostic checks, such as examining the histogram of weights (not shown), may be valuable protections against degeneracy.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{grid-sketch}
    \caption{
        Schematic of \gls{sow} weighting scheme defined in \cref{eq:weights}.
        This method is illustrated for a hypothetical target distribution (orange line) and $J=4$ samples $\psi_1, \psi_2, \psi_3, \psi_4$ (orange dots).
        As shown in \cref{eq:weights}, the weights $w_j$ (vertical lines) are calculated based on the cumulative distribution function of the target distribution  (black line) at the halfway points $\frac{1}{2}\qty[\psi_i+\psi_{i+1}]$ (vertical dashed lines).
        % TODO: add the weights to the legend
        % TODO: clarify colors! (NO, Klaus, you're not color blind -- but I am a little bit)
    }\label{fig:grid-sketch}
\end{figure}

The aim of this re-weighting framework is to integrate an ensemble of \glspl{sow} used for exploratory modeling into formal decision analysis, even when the \glspl{sow} deliberately over- or under-sample some regions of the parameter space.
As in \cref{sec:analysis-condition}, we must condition on a model: where the analysis of \cref{sec:analysis-condition} conditions upon deep uncertainties, the approach outlined in this subsection synthesizes across them.
We reiterate that stakeholders and experts will not, in general, agree on $p_\mathrm{belief}$ because its there is no ``true'' value of $p_\mathrm{belief}$ that could be objectively ascertained \citep{oreskes_verification:1994,walker_deep:2013}.
However, we posit that since we cannot be ``right,'' it is valuable to maximize the transparency of our implicit probabilistic assumptions, and suggest that writing down an explicit model for $p_\mathrm{belief}$ supports this aim.

\section{Demonstrating the concept with a case study}\label{sec:case-study}

To illustrate the proposed decision analytic framework, we model a one-time decision of whether to elevate a house, and if so by how much (\cref{fig:xlrm}).
We focus on a case study of a \emph{hypothetical} house in Norfolk, VA.
For interpretability, we focus on deep uncertainty in \gls{msl} and approximate other model parameters as shallow uncertainties as shown in \cref{tab:uncertainties}.
We use the notation developed in the previous section to describe the case study.
Specifically,
\begin{enumerate}
    \item The decision vector $\vb{x}$ is comprised of discrete possible house heightenings ($\Delta h$); we consider $\Delta h = \qty{\SI{0}{ft}, \SI{0.25}{ft}, \ldots, \SI{12}{ft}}$ f
    \item The \glspl{sow} describe annual time series of \gls{msl} over the $T=70$ year house lifetime so $\vb{s} \in \mathbb{R}^T$
    \item The system model $f$ quantifies up-front costs and lifetime expected damages, given a decision $x_i$ and \gls{sow} $s_j$, by integrating economic and engineering damage models over a probability distribution for storm surge.
\end{enumerate}
In the remainder of this section we describe data sources and treatment of \gls{slr} (\cref{sec:case-slr}), storm surge (\cref{sec:case-surge}), damages and metrics (\cref{sec:case-metrics}), and finally the subjective priors $p_\mathrm{belief}$ used to apply the re-weighting method described in \cref{sec:analysis-synthesize} to this case study (\cref{sec:case-priors}).

\begin{table}
    \centering
    \caption{
        Summary of parameters, their notation, and how their uncertainty is represented.
        Symbols describing the decision-analytic framework are described in \cref{fig:flowchart}.
    }\label{tab:uncertainties}
    \footnotesize
    \begin{tabular}{p{1.25in} p{0.75in} p{3in}}
        \toprule
        Name                 & Symbol            & Uncertainty                                                                          \\
        \midrule
        \Gls{msl}            & $\overline{y}(t)$ & Deeply uncertain: four physical models $\times$ four \acrshort{rcp} scenarios        \\
        Storm surge          & $y'(t)$           & Probabilistic: Bayesian inference on a stationary \acrshort{gev} model               \\
        Annual maximum flood & $y(t)$            & Deterministic: $y(t)=\overline{y}(t)+y'(t)$                                          \\
        Discount rate        & $\rho$            & Determinitic: 2.5\% per year                                                         \\
        Depth-damage         & $D(h-y)$          & Deterministic: based on HAZUS model \citep[see][]{zarekarizi_suboptimal:2020}        \\
        Elevation cost       & $C(\Delta h)$     & Deterministic: a piecewise linear model following \citet{zarekarizi_suboptimal:2020} \\
        Initial height       & $h_0$             & Deterministic: \SI{1}{ft} below the \gls{bfe}, unless otherwise noted                \\
        House floor area     & --                & Deterministic: \SI{1500}{ft^2}                                                       \\
        Structural value     & --                & Deterministic: \usd{200000}                                                          \\
        House lifespan       & $T$               & Deterministic: 70 years                                                              \\
        \bottomrule
    \end{tabular}
    %TODO: check if there are other symbols used
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{xlrm.pdf}
    \caption{
        Conceptual diagram of the considered example.
        A \acrfull{sow} consists of a description of the uncertain factors (red).
        We model a problem with a single lever (yellow), which is how high to elevate a house ($\Delta h$).
        For each \acrshort{sow} (red) and each value of $\Delta h$, the system model (blue) is used to calculate performance metrics (gray).
    }\label{fig:xlrm}
\end{figure}

\subsection{\Glsentrylong{slr}}\label{sec:case-slr}

We analyze simulations of \gls{msl} at Sewells Point, VA from four probabilistic physical models using data published in \citet{ruckert_coastal:2019}.
The four models considered are (i) the BRICK model (version 0.2) with slow (``BRICK Slow'') and (ii) fast (``BRICK Fast'') ice sheet dynamics \citep{wong_brick0.2:2017}, (iii) the \citet{kopp_probabilistic:2014} model (``K14''), and (iv) the \citet{deconto_antarctica:2016} model (``DP16'').
The \citet{kopp_probabilistic:2014} and \citet{deconto_antarctica:2016} models have a ten year time step, which we linearly interpolate onto a one year time step for consistency.
These four models represent physical processes, particularly of ice sheet dynamics, in different ways, leading to diverging sensitivity of \gls{msl} to forcing.
For a discussion of these model outputs we refer the reader to \citet{ruckert_coastal:2019}.

Estimates of nonstationary \gls{msl} also depend on anthropogenic forcing, which is itself deeply uncertain \citep{ho_scenarios:2019,srikrishnan_probabilistic:2022}.
To sample this uncertainty, we use simulations from each physical model under four \gls{rcp} scenarios, yielding sixteen time-varying distributions of \gls{msl}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{lsl-evolution}
    \caption{
        Projections of future mean sea level depend strongly on the choices of physical model and forcing.
        (A): 90\% confidence intervals for mean sea level at Sewells Point, VA as a function of time for a representative subset of three probabilistic models (out of sixteen).
        (B): probability distribution of \gls{msl} at Sewells Point, VA in the year 2100 for each probabilistic model considered.
    }\label{fig:lsl-evolution}
\end{figure}

The choices of physical model and \gls{rcp} scenario jointly determine future \gls{msl} $p(\overline{y}|t)$.
\Cref{fig:lsl-evolution}(a) shows the time-varying 90\% credible intervals of \gls{msl} for three representative models.
The divergence between the the best-case (blue) and worst-case (red) models is small in the early 21st century and increases rapidly thereafter.
\Cref{fig:lsl-evolution}(b) shows the \glspl{pdf} of mean sea level in 2100 (dashed vertical line in panel (a)) under each of the sixteen models considered.
We return in \cref{sec:results-conditional} to the problem that these multiple models poses to decision makers.

\subsection{Storm surges}\label{sec:case-surge}

Following prior work \citep[\eg,][]{garner_slrise:2018,sriver_sealevel:2018}, we model annual maximum floods $y(t)$ as the sum of sea level $\overline{y}(t)$, described in the previous subsection, and annual maximum storm surges $y'(t)$, neglecting hydrodynamic effects.

Data on storm surge comes from Sewells Point, VA (gauge 8638610) from the freely available NOAA tides and currents data archive \citep{noaa_tidesandcurrents:2022}.
Hourly recordings of water level are available from 1928 to the present; we use data from the period January 1, 1928 to December 31, 2021.
For each calendar year we first remove the annual mean, then calculate the maximum water level.
We refer this time series of annual maximum storm surges as $y'(t)$.
We display this time series of annual maxima storm surges in \cref{fig:surge-obs-return}(a).
The largest recorded surge was the Chesapeake-Potomac hurricane of 1933, which caused a surge of over \SI{7}{ft} at this gauge, but other hurricanes and Nor'easters have caused surges above \SI{6}{ft}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-obs-return}
    \caption{
        Annual maximum storm surges (after subtracting \acrlong{msl}) at Sewells Point, VA from the freely available NOAA tides and currents data archive \citep{noaa_tidesandcurrents:2022}.
        (A):
        time series of historic storms.
        Red (yellow) arrows denote notable tropical cyclones (Nor'easters).
        (B):
        return periods.
        Dots indicate observed values; their $x$-value is their plotting position using the Weibull formula (eq.~\ref{eq:weibull}).
        Gray lines show the 50, 80, and 95\% posterior confidence intervals from the Bayesian \gls{gev} fit (\cref{sec:case-surge}).
        % TODO: change caption for Obs (Weibull Plot Pos.) => Observations
        % TODO: make the different confidence intervals slightly different colors instead of changing opacity
        % TODO: move (B) to more logical location
    }\label{fig:surge-obs-return}
\end{figure}

We model future storm surge using a stationary \gls{gev} model:
\begin{equation}\label{eq:surge-model}
    y'(t) \sim \text{GEV}\qty(\mu, \sigma, \xi),
\end{equation}
where $y'(t)$ is the storm surge (above \gls{msl}) in year $t$ and a \gls{gev} distribution with location, shape, and scale parameters $\mu$, $\sigma$, and $\xi$, respectively, has the probability density function given in \cref{eq:gev-dist}.
This model assumes stationarity, neglecting any potential time dependence.

Our approach to model assessment is based on the concept of principled workflow design for model building and checking \citep[see][for details]{gelman_workflow:2020}.
One model choice, analogous to the choice of statistical distribution or the assumption of stationarity, is the choice of how to represent prior information.
We include two forms of prior information.
First, we constrain the shape parameter to be positive, $\xi > 0$, to reflect knowledge about the support of $y'$, which for a variable distributed according to \cref{eq:gev-dist} is:
\begin{equation*}
    \mathrm{supp}~ y' =
    \begin{cases}
        \xi < 0: & y' \in \qty(-\infty,~ \mu - \nicefrac{\sigma}{\xi}) \\
        \xi > 0: & y' \in \qty(\mu-\nicefrac{\sigma}{\xi},~ \infty).
    \end{cases}
\end{equation*}
Since storm surges cannot be negative, only the latter is physically defensible, justifying our choice to constrain the shape parameter to be positive.
Second, we add weakly informative priors.
Rather than applying prior information directly over the joint distribution of the parameters $\qty{\mu,\sigma,\xi}$, we instead apply a prior over extreme quantiles of the distribution, as in \citet{coles_evd:1996}.
Specifically, we apply Inverse Gamma priors over the 2, 10, 100, and 500 year return levels, with means of \SIlist{4;6;10;15}{ft} and standard deviations of \SIlist{1.5;1.75;2.25;2.75}{ft}, respectively.
The parameters of the Inverse Gamma distribution can be calculated from these moments (see eq.~\ref{eq:inv-gamma-params}).
These means and standard deviations were chosen to represent plausible physical ranges (\cref{fig:surge-gev-priors}).

For inference, we draw \num{10000} samples from the posterior distribution $p(\mu,\sigma,\xi | y')$ using Hamiltonian Markov Chain Monte Carlo \citep{Betancourt:2017vd,hoffman_nuts:2011} implemented in the Turing package of the Julia programming language \citep{perkel_julia:2019,ge_turing:2018,tarek_dynamicppl:2020,besancon_distributions.jl:2021,bezanson_julia:2012}.
Diagnostics suggest (though cannot guarantee) convergence (see \cref{tab:surge-posterior-mcmc-diagnostics}).
We evaluate the model's fit using posterior predictive checks \citep[see][section 2.4 and references therein]{gelman_workflow:2020}.
Using the lag 1 and 2 partial autocorrelations, sample maximum, sample minimum, sample median, and Mann-Kendall test value as Bayesian test statistics, we find that draws from the posterior predictive distribution match the observed test statistics credibly (\cref{fig:surge-test-statistics}) although panels (a) and (b) suggest the possibility of temporal structure not captured by our stationary \gls{iid} model.
Future efforts could represent this structure by conditioning the parameters of the distribution on relevant climate indices \citep[as in][]{wong_surge:2018,farnham_orbrep:2018,farnham_jetstream:2017,ossandon_semibayesian:2021}.

Other model validations lend confidence to the stationary \gls{gev} model selected.
For example, \cref{fig:surge-obs-return}(b) shows the estimated return periods for these storm surges; the estimated return period of the (shading) matches the (dots) the empirical plotting position dots.
Further, a positive control test (\cref{fig:surge-synthetic-data-experiment}) validates the model's ability to recover known parameter values.

\subsection{Damages and metrics}\label{sec:case-metrics}

The system model (``relationships'' in \cref{fig:xlrm}) is comprised of two key pieces.
The first is a fragility model that estimates the expected flood damages for a particular year (``expected annual damages''), given the elevation of the house and the mean sea level for that year.
The second model converts a time series of annual expected damages into lifetime expected damages.

We define expected annual damages in year $t$ as the expectation of the damage function with respect to storm surge.
This expectation depends on the house's height ($h = h_0 + \Delta h$) where $h_0$ is the initial height relative to the gauge and $\Delta h$ is the amount by which the house is elevated.
The expected annual damage is thus
\begin{equation}\label{eq:ead}
    \textrm{EAD}(t) = \mathbb{E}[D \qty(h-\overline{y}(t))] = \int_{y'} p(y') D \qty(h - \qty(\overline{y}(t) + y')) \dd{y'},
\end{equation}
where $D(h-y)$ is a deterministic function specifying damage as a function of flood depth (relative to the house).
Following \citet{zarekarizi_suboptimal:2020}, we use the we use the \gls{hazus} depth-damage curves provided by \gls{fema}; this depth-damage relationship is shown in \cref{fig:cost-depth-damage}.
For comparison, \cref{fig:cost-depth-damage} also shows the ``Europa'' depth-damage relationship developed by the Joint Research Center of the European Commission's science and knowledge service \citep{huizinga_depthdamage:2016}.
Although \citet{zarekarizi_suboptimal:2020} demonstrate that the choice of fragility function is important for informing house elevation, we use only the HAZUS model for simplicity.

The expected annual damage is sometimes calculated by assuming analytically tractable functional forms for the depth-damage relationship and for the  distribution of hazard \citep[\eg][]{vandantzig_dike:1956}.
However, the convolution of the HAZUS depth-damage equation with the \gls{gev} posterior does not have a tractable analytic solution so we instead estimate it through a Monte Carlo method (see \cref{sec:alg-ead} for details).
Then, because the expectation in \cref{eq:ead} depends only on $h-\overline{y}(t)$, we calculate expected annual damages for a wide range of possible heights, then use this to train a computationally efficient surrogate model (using linear interpolation; see \cref{sec:surrogate-ead}).

The second component of the system model converts a time series of $\mathrm{EAD}$ into lifetime expected damages, which we define as the up-front  discounted sum of expected annual damages:
\begin{equation}\label{eq:led}
    \textrm{LED} = \sum_{t=t_i}^{t_f} \gamma^{\qty(t-t_i)} \textrm{EAD}(t),
\end{equation}
where  $\gamma = 1- \rho$ ($\rho$ being the discount rate), the initial time $t_i=2022$, and the end time $t_f = t_i + T - 1$.
Although \citet{zarekarizi_suboptimal:2020} show that uncertainty in the discount rate is important for decision support, we use a fixed discount rate (see \cref{tab:uncertainties}) for the purposes of this didactic study.
For a more theoretical discussion see \citet{arrow_discount:2013}.

To assess the performance of a given decision for a specific \gls{sow} (``Metrics'' in \cref{fig:xlrm}), we calculate the following metrics for each decision-\gls{sow} combination:
\begin{enumerate}
    \item ``Up-front cost'' is the cost of elevating a house. Following \citet{zarekarizi_suboptimal:2020}, we use estimates of construction cost from the Coastal Louisiana Risk Assessment \citep{fischbach_clara:2012}. We normalize this cost by house value. This cost curve is shown in \cref{fig:cost-up-front}.
    \item ``Lifetime expected damages'' is calculated following \cref{eq:led}.
    \item ``Expected lifetime costs'' is the sum of lifetime expected damages and up-front costs.
\end{enumerate}

\subsection{Prior over \glsentrylong{slr}}\label{sec:case-priors}

We construct three probabilistic models for $p_\mathrm{belief}(\psi)$, which represents the amount of \gls{slr} from 2022 to 2100.

We use a Gamma distribution for all three priors, parameterized following \cref{eq:gamma-dist}.
\Cref{tab:slr-priors} specifies the parameters of these distributions, as well as some quantiles of the distributions.
Their \glspl{pdf} are also plotted in \cref{fig:lsl-priors-weights}(A).

\begin{table}[h]
    \centering
    \caption{
        Subjective priors over \gls{slr} from 2022 to 2100, \ie $p_\mathrm{belief}(\psi)$.
        The name of the distribution, the parameters of the Gamma distribution with shape $\alpha$ and scale $\theta$, and the 2.5, 25, 50, 75, and 97.5th percentiles (values in \si{ft}).
    }\label{tab:slr-priors}
    \input{../../plots/lsl_priors.tex}
    % TODO: add another column: Parameter and Quantiles [in feet]
\end{table}

We developed these priors for didactic purposes, to illustrate a range of possible beliefs.
We can compare them, for example, with analysis published by \gls{noaa}, which project \SIlist{1.94;2.62;4.27;5.25;6.89}{ft} for the low, intermediate, low intermediate, intermediate high, and high scenarios, respectively \citep[table.~2.4]{sweet_slr:2022}.
We can also compare to the analyses of \citet{sriver_sealevel:2018} which use a rescaled Beta distribution with bounds of \SIrange{255}{2508}{\milli\meter} (\SIrange{0.83}{8.2}{ft}) and a most plausible estimate of \SI{950}{\milli\meter} (\SI{3.1}{ft}).
Our samples bound all of these estimates.

\section{Results and discussion}\label{sec:results}

We illustrate our approach to synthesizing uncertainties by sequentially analyzing the house elevation problem through the lenses of exploratory modeling (\cref{sec:results-exploratory}), scenario-conditional analysis (\cref{sec:results-conditional}), and finally the proposed synthesis method (\cref{sec:results-synthesis}).

\subsection{Exploratory modeling}\label{sec:results-exploratory}

We begin by using our model in an ``exploratory'' mode with an aim of learning about interactions between system dynamics and decisions.

\begin{figure}
    \includegraphics[width=\textwidth]{scenario-map-slr-cost}
    \caption{
        Scenario maps show the dependence of expected lifetime cost (damages plus up-front cost) as a function of \gls{msl} in 2100 for several values of initial height ($h_0$) and house elevation ($\Delta h$).
        Colors indicate the density of \glspl{sow}; the color of each grid box corresponds to the number of \glspl{sow} falling within that box.
        The lowest-cost outcomes occur when both exposure is low ($h_0$ is large and \gls{slr} is minimal) and the house is not elevated (no up-front cost).
        The highest-cost outcomes arise when exposure is high ($h_0$ is small and \gls{slr} is rapid) and investment is inadequate.
        In all cases, elevating the house reduces the variance in total lifetime cost.
        Values are sensitive to model constants; see \cref{tab:uncertainties}.
    }\label{fig:scenario-map-slr-cost}
\end{figure}

One application of explotatory analysis is to reveal the range and variation in outcomes, conditional on taking a particular decision.
\Cref{fig:scenario-map-slr-cost} shows the dependence of expected lifetime costs (damages plus up-front costs; $y$-axis) as a function of  \gls{slr} over the house lifetime ($x$-axis), height increase ($\Delta h$; columns), and initial elevation ($h_0$; rows).
The outcomes with lowest total lifetime costs arise when the house is not elevated ($\Delta h = 0$) and \gls{slr} is minimal (bottom left corners).
The outcomes with highest total lifetime costs arise when the house is elevated only slightly and \gls{slr} is rapid.
As $\Delta h$ increases, the best-case scenario becomes more expensive because up-front costs increase, but worst-case scenarios become less expensive because even if \gls{slr} is substantial, damages will be negligible.

This analysis answers ``what-if'' questions like ``given $h_0$ and $\Delta h$, what is the range of total costs a homeowner could face if \gls{slr} over the house lifetime is \SI{1}{ft} or \SI{10}{ft}.''
For some decision-makers, contextualizing this information against a few scenarios of \gls{slr} \citep[\eg, those of][]{sweet_slr:2022} may prove sufficient for decision making.
This would be analogous to decision scaling methods that plot climate model projections on a scenario map of water system performance as a function of changes in key climate variables \citep{Brown:2012kb,Steinschneider:2015kk}.
Scenario mapping is also commonly used in \gls{rdm} analyses \citep{lempert_rdm:2019,sriver_sealevel:2018}.
However, this analysis does not shed light on cost-benefit analyses or return periods, nor does it permit quantitative comparison against other possible decisions unless strong implicit assumptions are made.

\subsection{Scenario-conditional optimization}\label{sec:results-conditional}

We now turn to the scenario-conditional analysis described in \cref{sec:analysis-condition}.
Whereas the exploratory analysis of the previous subsection interpreted each time series of future sea level as a sample from the space of possible futures, we can also interpret each \gls{sow} as a draw from one of the sixteen models of \gls{slr} shown in \cref{fig:lsl-evolution}.
As discussed in \cref{sec:analysis-condition}, this allows a formal estimation of decision metrics.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{tradeoffs-by-rcp}
    \caption{
        Each probabilistic model or scenario leads to a different estimate of the Pareto frontier.
        For emphasis, we highlight three representative models: the Brick Slow model \citep{wong_brick0.2:2017} under \gls{rcp} 2.6, the K14 \citep{kopp_probabilistic:2014} model under \gls{rcp} 6.0 and the DP16 model \citep{deconto_antarctica:2016,kopp_evolving:2017} under \gls{rcp} 8.5.
        (A): trade-off between up-front cost (which is a monotonic function of height increase) and expected lifetime costs.
        (B): trade-off between up-front cost and lifetime expected damages (eq.~\ref{eq:led}).
        Light gray lines show estimates for all 16 models (four \gls{rcp} scenarios times four physical parameterizations) considered.
        Colored lines highlight three representative models for emphasis.
        % TODO: add arrows and star
    }\label{fig:tradeoffs-by-rcp}
\end{figure}

This probabilistic interpretation allows us to compute, for example, expected values.
For example, \cref{fig:tradeoffs-by-rcp}(a) plots the expected total lifetime cost as a function of $\Delta h$ for the sixteen models considered (we highlight three representative models).
Similarly, \cref{fig:tradeoffs-by-rcp}(b) plots the lifetime expected damages as a function of $\Delta h$.
For small $\Delta h$, expected costs are low under optimistic models (\eg, \gls{rcp} 2.6 with slow ice sheet dynamics; red lines) and high under pessimistic models (\eg, \gls{rcp}8.5 with the DP16 model; blue lines).
For example, under the most pessimistic model (blue line), the cost-minimizing height increase is \SI{6}{ft}, which incurs an up-front cost of 73\% of the house value but reduces lifetime expected damages by over 150\% of house value.
Converseley, under the most optimistic model (gray line), the cost-minimizing decision is to not elevate, as elevating \SI{6}{ft} incurs the same up-front cost of 73\% of house value yet reduces lifetime expected damages by less than 50\% of house value.

This approach is, in a sense, another form of exploratory modeling: instead of considering a very large ensemble of \glspl{sow}, we consider a much smaller set of probabilistic models.
This approach is attractive because it allows modelers to focus on their domain expertise (\eg, the response of ice sheets and global sea levels to a particular climate future).
However, conditioning simulations on a set of climate futures and physical models presents what we term ``the multiple \gls{pdf} problem'' because it leaves decision makers with many \glspl{pdf} to choose from.
The multiple \gls{pdf} problem has also been shown in other contexts.
For example, \citet{sharma_rcp:2021} model the reliability of stormwater infrastructure under different climate models and downscaling methods, finding diverging estimates of future rainfall hazard, even under a single \gls{rcp} scenario.
Similarly, \citet{wong_nola:2017} construct 18 probability distribution functions for future flood risk in New Orleans, considering multiple models for ice sheet dynamics and storm surge and multiple \gls{rcp} scenarios.
And \citet{lempert_rdm:2019}
Although this scenario-conditional analysis is appropriate for understanding differences between models, its key limitation is that it \textbf{places the burden for deciding which model to design for onto the end user.}

\subsection{Synthesizing deep uncertainties for decision analysis}\label{sec:results-synthesis}

The proposed approach can help overcome the limitations of scenario-conditional analysis.
We illustrate how the re-weighting method described in \cref{sec:analysis-synthesize} can shed light on climate risk management under deep uncertainty.
We present results using each of the models for $p_\mathrm{belief}$ outlined in \cref{sec:case-priors}.
These three distributions are shown in \cref{fig:lsl-priors-weights}(A).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{lsl-priors-weights}
    \caption{
        Impact of subjective priors for local sea level on implicit weight given to each \gls{rcp} scenario and physical model.
        We develop three distributions (``subjective priors'') representing plausible probabilistic beliefs about \gls{msl} at Sewells Point, VA in 2100, relative to the present.
        The \glspl{pdf} of these subjective priors are shown in panel (A).
        In panels (B-C) we show the relationship between these subjective priors and the 16 probabilistic models (four \gls{rcp} scenarios and four physical representations) available.
        Specifically, (B-C) show the average weight given to each model by each of the subjective priors.
    }\label{fig:lsl-priors-weights}
\end{figure}

One application of this method is to diagnose which assumptions different $p_\mathrm{belief}$ are consistent with.
\Cref{fig:lsl-priors-weights}(B-D) shows the average weight that each prior assigns to \glspl{sow} generated by each \gls{rcp} scenario and physical model.
For example, the rapid \gls{slr} scenario (green line in \cref{fig:lsl-priors-weights}) places most weight on the DP16 model, and particularly on \gls{rcp} 8.5 which by some accounts is unlikely given current policy \citep{hausfather_scenarios:2020,srikrishnan_probabilistic:2022}.
Conversely, the slow \gls{slr} scenario (red line) places most weight on the BRICK models, particularly \gls{rcp} 2.6 (also unlikely given current policy) and \gls{rcp} 4.5.
The uncertain \gls{slr} scenario (blue line) places approximately equal weight across models.

This method can also be used to calculate expectations, allowing us to revisit the trade-off diagrams of \cref{fig:tradeoffs-by-rcp}.
\Cref{fig:tradeoffs-by-prior} shows the total lifetime cost (panel A) and lifetime expected damages (panel B) under each model.
Notably, they give different guidance.
Under an assumption of rapid \gls{slr}, elevating the house by approximately \SI{6}{ft} costs 73\% of house value and reduces damages by over 100\% of house value, yielding a benefit to cost ratio of approximately 1.25.
Under an assumption of slow \gls{slr}, elevating the house by \SI{6}{ft} reduces damages only by 50\% of house value, yielding a benefit to cost ratio of approximately 0.7.
Under the intermediate / uncertain \gls{slr} assumption, the expected lifetime costs are similar for elevating or not elevating the house, and thus the benefit to cost ratio is nearly 1.
Under all assumptions, elevating by only a few feet is impractical because it involves paying the large fixed costs of elevation (\cref{fig:cost-up-front}) but offers relatively little flood reduction.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{tradeoffs-by-prior}
    \caption{
        As \cref{fig:tradeoffs-by-rcp}, but with Pareto frontiers for the full distribution of outcomes using the three models of $p_\mathrm{belief}$ (colors).
        Thin gray lines show the trade-off curves for each \gls{rcp} scenario and model separately as in \cref{fig:tradeoffs-by-rcp}.
        % TODO: maybe we don't need the gray lines, make the red/yellow/blue lines more bold.
    }\label{fig:tradeoffs-by-prior}
\end{figure}

\section{Conclusions}\label{sec:conclusions}

This study develops a framework to increase the transparency of assumptions about the likelihood of different \glspl{sow}.
Additionally, we develop a framework capable of blending iterative, stakeholder-driven exploratory modeling with subjective probabilistic expert assessment.
Such an approach is urgently needed given that deeply uncertain nonstationarity hazards pose a fundamental challenge to classical methods of hazard estimation.
We use a didactic case study of house elevation in the coastal zone to illustrate a method for transparently synthesizing across deep uncertainties.

There are several limitations to our study that merit further discussion.
The first category has to do with limitations of the underlying methodology proposed for reweighting state of the world.
For example, we develop a subjective prior belief $p_\mathrm{belief}(\Psi)$ over \gls{msl} in the year 2100.
Although this is a low-dimensional representation of the full time series, it is not a sufficient statistic; prior studies have shown that using Approximate Bayesian Computation to calibrate models on low dimensional statistics using that are not sufficient statistics can lead to biased estimates of the posterior \citep{csillery_abc:2010,marjoram_abc:2006}.
Although we are not performing calibration here, and this is thus not a direct concern, time series with the same \gls{msl} in 2100 may differ in other ways, and experts may have prior information about the likelihood of these differences not represented in our model.
A related concern is that we developed our three distributions for $p_\mathrm{belief}(\Psi)$  in an \emph{ad hoc} fashion that may not represent well-calibrated beliefs.
Although this is appropriate for our didactic illustration, recent advances in Bayesian elicitation of expert opinion \citep[see][and references therein]{mikkola_elicitation:2021} can be applied to improve decision making in real world case studies.
More fundamentally, our method assumes that there exists an expert capable of integrating over the many processes that drive \gls{slr}, from global greenhouse gas emissions to the global carbon cycle to climate sensitivity and ice sheet response \citep{morgan_elicitation:2014}.
An alternative approach would be to build a probabilistic model for each of these steps, and to use each as input to the next to develop a fully probabilistic model for \gls{slr}.
Yet while some progress has been made developing probabilistic models for specific elements of this model chain \citep[\eg,][]{srikrishnan_probabilistic:2022,wong_brick0.2:2017}, this remains a computational and conceptual challenge.
Finally, while our aim in this paper has been to demonstrate the value of integrating Bayesian workflow \citep{gelman_workflow:2020} into \gls{dmdu}, further work is needed to improve this integration.

The second category of limitations has to do with the case study and our interpretation of the house elevation decision problem.
This problem intersects with decisions about where to live and how to manage household finances, both of which are highly complex.
One extension of our analysis would be to consider additional decision objectives.
In particular, we hypothesize that incorporating improved representations of risk aversion into decision support may substantially improve their usability.
One could also extend the analysis to consider additional sources of uncertainty such as depth-damage relationships \citep{Rozer:2019,nofal_fragility:2020}, the cost of elevating a house, the house lifespan, the effective discount rate, and value of the land on which the house is built \citep[provides a framework for addressing some of these]{zarekarizi_suboptimal:2020}.
Finally, while here we consider the decision to be a one-time decision, one could also frame this as problem a sequential decision.
The aaalysis of sequential decision problems applies tools from control theory and reinforcement learning to identify policies that map ``triggers'' (\ie, state variables) to decisions \citep{herman_control:2020}.
Yet although framing the decision through a sequential lens can increase adaptability and improve outcomes \citep{fletcher:2017,garner_slrise:2018}, the optimized policy rules are necessarily sensitive to the characterization of uncertainty, and thus the problem of synthesizing across deep uncertainties remains \citep{herman_control:2020}.

These limitations motivate several directions for future research.
From a methodological perspective, developing model chains that capture uncertainties in global energy and economic pathways, global climate sensitivity, and local hazard response \citep[see fig.~1 of][]{moss_uncertainties:2000} offers a principled framework for fully probabilistic estimates of local hazard, subject to (still necessarily subjective) priors over key parameters.
From a decision support perspective, improved understanding of the conditions under which household-scale strategies for flood risk management, like elevation, achieve relevant objectives could support improved resilience and adaptation.
Additionally, since developing bespoke analyses for each house may be impractical, identifying decision rules that scale to different house characteristics may add value.

The proposed scenario weighting framework can be applied to inform critical challenges in climate risk management.
An obvious area of application is to the design of infrastructure for climate risk management.
For example, much of the stormwater infrastructure in the United States is inadequate for current climate \citep{lopez-cantu:2018}, let alone future changes.
Yet upgrading this infrastructure is costly and subject to large uncertainties between rainfall models \citep{sharma_rcp:2021} and \gls{rcp} scenarios.
Similarly, decisions like  levee heightening \citep{garner_slrise:2018,oddo_coastal:2017,vandantzig_dike:1956} and sea wall design (as discussed in the Introduction) are subject to deep uncertainties in sea level rise.
This framework can also be applied to uncertainties in non-physical variables.
For example, investments in water resources planning and management depend on assumptions of future water demand, availability, and technologies \citep{trindade_deeplyuncertainpathways:2019}.
Similarly, analyses of climate change mitigation options, such as estimates of the social cost of pollutants \citep{errickson_methane:2021} or cost-minimizing energy transition pathways, are conditional on probabilistic models for inputs like technology prices and population.

Of course, all models are ultimately wrong \citep{box:1976}.
Thus seeking decisions that perform well across a range of assumptions, and improving the deccision space through robust design and flexibility, can improve outcomes.
Yet whenever decisions are compared quantitatively, assumptions about the probability of different possible futures are necessarily made.
We call for researchers studying climate risk management to make these implicit assumptions explicit, and we suggest that coordinated guidance can help practitioners determine better design criteria.

\section*{End Matter}

\subsection*{Acknowledgements}

This work was supported by \acrfull{noaa} through the Mid-Atlantic Regional Integrated Sciences and Assessments (MARISA) program under NOAA grant NA16OAR4310179 and by the Penn State Center for Climate Risk Management.
JDG thanks Rice University for support.
KK thanks Dartmouth College for support.
The authors thank Tor Erlend Fjelde and Vivek Srikrishnan for helpful comments.

\subsection*{Code and data availability}

All code, including source code, is available under the GNU Public License (version 3) at \url{https://github.com/jdossgollin/2021-elevation-robustness}.
This code is written in the open source Julia programming language.

\subsection*{Author Contributions}

JDG and KK designed the research.
JDG wrote the codes and ran simulations.
JDG wrote the manuscript in consultation with KK.
JDG and KK revised the manuscript.

\printbibliography

\appendix
\newcommand{\hbAppendixPrefix}{S}
\renewcommand{\thefigure}{\hbAppendixPrefix\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{\hbAppendixPrefix\arabic{table}}
\setcounter{table}{0}
\renewcommand{\theequation}{\hbAppendixPrefix\arabic{equation}}
\setcounter{equation}{0}

\newpage

\printglossary[type=\acronymtype,title=List of Abbreviations]

\section{Supplemental methods}

\subsection{Statistical distributions}

To avoid ambiguity, we define here some statistical distributions used in the main text.
Except where otherwise indicated, we follow the default notation of the Distributions.jl Julia package \citep{besancon_distributions.jl:2021}.

\subsubsection{Generalized Extreme Value distribution}
We parameterize the \gls{gev} distribution as
\begin{equation}\label{eq:gev-dist}
    f(x | \mu, \sigma, \xi)= \begin{cases}
        \frac{1}{\sigma} \qty[ 1 + \qty( \frac{x-\mu}{\sigma} ) \xi ]^{-1 / \xi - 1} \exp \qty{- \qty[1+\qty(\frac{x-\mu}{\sigma}) \xi ]^{-1 / \xi} }, & \xi \neq 0 \\
        \frac{1}{\sigma} \exp \qty{-\frac{x-\mu}{\sigma}} \exp \qty{ -\exp \qty[-\frac{x-\mu}{\sigma}]},                                               & \xi=0.
    \end{cases}
\end{equation}

\subsubsection{Inverse Gamma distribution}
Parameterizing the Inverse Gamma distribution as
\begin{equation}\label{eq:inv-gamma-dist}
    f(x | \alpha, \theta) = \frac{\theta^\alpha x^{-(\alpha + 1)}}{\Gamma(\alpha)} e^{-\frac{\theta}{x}}, \quad x > 0,
\end{equation}
the parameters $\alpha$ and $\theta$ can be computed from the desired mean $\mu$ and standard deviation $\sigma$ as
\begin{align}\label{eq:inv-gamma-params}
    \begin{split}
        \alpha &= 2 + \frac{\mu^2}{\sigma^2} \\
        \theta &= \mu \qty(\alpha - 1).
    \end{split}
\end{align}

\subsubsection{Gamma distribution}
We parameterize the Gamma distribution as
\begin{equation}\label{eq:gamma-dist}
    f(x | \alpha, \theta) = \frac{x^{\alpha-1} e^{-x/\theta}}{\Gamma(\alpha) \theta^\alpha},
    \quad x > 0.
\end{equation}

\subsubsection{Plotting position}
The plotting position used in \cref{fig:surge-obs-return} is the Weibull (``empirical'') plotting position
\begin{equation}\label{eq:weibull}
    \nicefrac{r}{N + 1}
\end{equation}
where $N$ is the sample size and $r$ is the order of the $N$ observations ($r=1$ is the largest, $r=N$ is the smallest).

\subsection{Algorithm to estimate expected damages}\label{sec:alg-ead}

As discussed in \cref{sec:case-metrics}, we use a Monte Carlo integration to estimate expected damages $D$ as a function of house elevation $h = h_0 + \delta h$ and \gls{msl}, $y(t)$.
Specifically, for $m=1, \ldots, M$:
\begin{enumerate}
    \item draw a sample from the posterior distribution of storm surge (see \cref{eq:surge-model})  $\qty{ \mu_m, \sigma_n, \xi_m}$
    \item simulate a single storm surge from this stationary \gls{gev} distribution and add the mean sea level to get total flood depth $y^\mathrm{sim}_m$
    \item calculate the flood damages for this draw by plugging the annual maximum flood depth relative to the house,$h - y'(t)$, into  the deterministic HAZUS depth-damage relationship, storing this as the $m$th damage.
\end{enumerate}
We then estimate expected annual damages as the sample mean of the $M$ estimates.

\subsection{Surrogate model for expected annual damages}\label{sec:surrogate-ead}
Evaluating expected annual damages for each of $J$ simulations of \gls{slr}, each of $N$ draws from the posterior distribution of storm surge, and each of $T$ time steps for $K$ models requires $N \times J \times T \times K$ simulations.
In our model, we have $T=70$, $N=\num{10000}$, $J=\num{179232}$, hence exhaustive sampling may be prohibitive.

Noticing that this function depends only on the elevation of the house relative to \gls{msl}, we develop a simple emulator for expected annual damages given this difference: $\hat{D}(h - \overline{y})$.
To do this, we  precompute expected annual damage for all height differences in \SI{0.25}{ft} increments from \SIrange{-30}{30}{ft} and fit a piecewise linear interpolation to this data.
We use $K=\num{1e6}$ samples to fit this emulator for each of the 241 increments.
This model is shown in \cref{fig:cost-expected-damage-emulator}.
Once this interpolation has been precomputed, calculating expected annual damage for a particular year only requires evaluating a piecewise linear function.

\section{Supplemental figures}

\begin{figure}
    \centering
    \includegraphics[width=4in]{cost-depth-damage}
    \caption{
        Depth-damage relationship.
        Following \citet{zarekarizi_suboptimal:2020}, we use the \gls{hazus} depth-damage curves.
        Since results are sensitive to choice of depth-damage equation, we illustrate (for comparison only) the ``Europa'' depth-damage relationship developed by the Joint Research Center (JRC) of the European Commission's science and knowledge service \citep{huizinga_depthdamage:2016}.
    }\label{fig:cost-depth-damage}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=4in]{cost-expected-damage-emulator}
    \caption{
        As discussed in \cref{sec:case-metrics}, we model expected annual damages (eq.~\ref{eq:ead}) as a function of the house's elevation relative to \gls{msl}.
        Damages ($y$ axis) are shown as a percentage of house value.
    }\label{fig:cost-expected-damage-emulator}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=4in]{cost-up-front}
    \caption{
        Following \citet{zarekarizi_suboptimal:2020}, we model the cost of elevating a single-family house by interpolating estimates from the Coastal Louisiana Risk Assessment Model \citep{johnson_clara:2013}.
        According to this model, the unit cost of elevating a house by 3-7, 7-10, and 10-14 feet is \usd{82.50}, \usd{86.25}, and \usd{103.75} per square foot, respectively, with a \usd{20745} initial cost.
        Values are sensitive to house floor area and structural value; see \cref{tab:uncertainties}.
    }\label{fig:cost-up-front}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=4in]{surge-gev-priors}
    \caption{
        Prior distributions for annual maximum storm surge.
        Rather than apply a prior over model parameters directly, we apply a weakly informative prior over quantiles of the resulting distribution (that is, over a function of the model parameters) following \citet{coles_evd:1996}.
        See \cref{sec:case-surge} for details.
        For the 2, 10, 100, and 500 year events we apply Inverse Gamma distributions, with means \SIlist{4;6;10;15}{ft} and standard deviations \SIlist{1.5;1.75;2.25;2.75}{ft}, respectively.
    }\label{fig:surge-gev-priors}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-prior-return}
    \caption{
        Implicit prior over storm surge represents large uncertainty but is consistent with basic physical reasoning (see \cref{sec:case-surge}).
        The prior median (blue line) and uncertainty quantiles (shading) of the return period for different return levels of storm surge at Sewells Point, VA using prior predictive sampling \citep{gelman_workflow:2020}.
        This illustrates the weakly informative prior information (\cref{sec:case-surge}) used in the analysis.
        These weakly informative priors were selected to be consistent with basic physical principles (\eg, storm surges most years are $>$ \SI{2}{ft} but rarely exceed \SI{20}{ft}).
        % TODO: fix legend to say Prior Median
    }\label{fig:surge-prior-return}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-synthetic-data-experiment}
    \caption{
        Synthetic data experiment as a positive control test for the \gls{gev} model of storm surge.
        A synthetic record (dots) was sampled from a \gls{gev} distribution with location, scale, and shape parameters of 4, 0.5, and 0.15, respectively.
        These samples were used to fit the Bayesian \gls{gev} model described in \cref{sec:case-surge}; the gray shading indicates the 50, 80, and 95\% posterior confidence intervals.
        The blue line shows the true quantiles of the (known) \gls{gev} distribution.
        By random chance the sample maximum has a true return period of $\gg 250$ years, which increases the upper confidence interval of the estimated return probabilities, but the true value is nevertheless within the 50\% posterior confidence interval.
        This experiment yields similar results for alternative values of the known \gls{gev} distribution, and for different random seeds (not shown).
    }\label{fig:surge-synthetic-data-experiment}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-posterior-chains}
    \caption{
        Markov chain diagnostic plots for posterior draws from the storm surge model.
        We draw \num{10000} samples by running four chains of \num{3500} iterations each and discarding the first \num{1000}.
        The mixing of the chains is consistent with, though does not guarantee, convergence.
    }\label{fig:surge-posterior-chains}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-prior-chains}
    \caption{
        As \cref{fig:surge-posterior-chains}, but for draws from the prior distribution.
    }\label{fig:surge-prior-chains}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-test-statistics}
    \caption{
        Posterior predictive checks for the stationary \gls{gev} storm surge model (\cref{sec:case-surge}).
        Each panel shows a different test statistic: partial autocorrelation at lags 1 and 2; sample maximum; sample minimum; sample median; and Mann-Kendall trend test statistic.
        The histograms show the distribution of each test statistic from the posterior predictive distribution.
        Orange lines show the test statistic's value in the observed data.
        Observed values near the mode of the posterior predictive distribution are consistent with, but do not guarantee, a good fit.
        For further discussion of posterior predictive checks, see Chapter 6 of \citet{gelman_bda3:2014}.
    }\label{fig:surge-test-statistics}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{scenario-map-height-slr}
    \caption{
        Expected total lifetime cost (damages plus up-front cost) as a function of \gls{slr} over the house lifetime and height increase $\Delta h$.
        Initial house elevation is fixed to \SI{1}{ft} below the \gls{bfe}.
        Expectations were computed for discrete values of $\Delta h$ ($x$ axis) by discretizing \glspl{sow} ($y$ axis), then taking the sample mean over each grid cell.
    }\label{fig:scenario-map-height-slr}
\end{figure}

\section{Supplemental tables}

\begin{table}[h]
    \centering
    \caption{
        Diagnostic statistics for the Hamiltonian Monte Carlo sampling for the storm surge posterior draws.
        Statistics include the mean and standard deviation of each parameter, the naive standard error and Monte Carlo standard error (which measure uncertainty in the mean), the effective sample size, $\hat{R}$ diagnostic, and effective samples per second, which describes sampling speed.
        The naive standard error (SE) returns the standard error of the mean.
        The Monte Carlo standard error (MCSE) is calculated using the initial monotone sequence estimator \citep[][pp.~473-483]{geyer_practical:1992}.
        The effective sample size (ESS) is a crude estimate of the number of independent samples and is calculated following \citet[][pp.~473-483]{geyer_practical:1992}.
        The $\hat{R}$ is an indicator of the convergence of the Markov chains to the target distribution; a value $\hat{R}$ value close to one is consistent with, though does not guarantee, convergence \citep{mcelreath_rethinking2:2020}.
        For additional details see the \texttt{MCMCDiagnosticTools} package documentation.
    }\label{tab:surge-posterior-mcmc-diagnostics}
    \input{../../plots/surge-posterior-mcmc-diagnostics.tex}
\end{table}

\begin{table}[h]
    \centering
    \caption{As \cref{tab:surge-posterior-mcmc-diagnostics} but for draws from the prior distribution.}\label{tab:surge-prior-mcmc-diagnostics}
    \input{../../plots/surge-prior-mcmc-diagnostics.tex}
\end{table}

% up to 1250 words
\end{document}
