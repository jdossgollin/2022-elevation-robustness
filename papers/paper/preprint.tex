\documentclass[12pt]{article}

% Figures
\usepackage{graphicx}
\usepackage[list=true]{subcaption}
\graphicspath{{../../plots/}{../../tikz/}{../../img/}}
\usepackage[section]{placeins} % require floats to appear in the section they are defined

% fonts and appearance
\usepackage{amsmath, amsfonts, physics, siunitx}
\usepackage[american]{babel} 
\usepackage[T1]{fontenc} % improved font encoding
\usepackage[ttscale=0.8]{libertine}
\usepackage{fontawesome5}
\usepackage[format=plain, textfont=it]{caption}

% page size and margins
\usepackage{geometry}
\geometry{letterpaper,top=1in, bottom=1in, left=0.5in, right=2in}

% footer
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[en-US]{datetime2}
\fancyhf{}
\fancyhead[L]{CONFIDENTIAL DRAFT -- Doss-Gollin \& Keller}
\fancyhead[R]{\DTMnow}
\fancyfoot[R]{page~\thepage~of~\pageref{LastPage}}
\pagestyle{fancy}

% TO DO NOTES
\usepackage{xcolor} % list of colors at https://en.wikibooks.org/wiki/LaTeX/Colors
\definecolor{giallo}{HTML}{F0BC42} % https://teamcolorcodes.com/a-s-roma-color-codes/
\definecolor{rosso}{HTML}{8E1F2F}
\definecolor{grigio}{HTML}{CACACC}
\definecolor{nero}{HTML}{000000}
\usepackage[textsize=scriptsize]{todonotes}
\setlength{\marginparwidth}{1.5in}
\newcommand{\james}[1]{\todo[color=giallo, textcolor=nero]{\textbf{ATTN James:~}#1}} % if desired create a custom command for each author
\newcommand{\klaus}[1]{\todo[color=rosso, textcolor=grigio]{\textbf{ATTN Klaus:~}#1}}

% better tables
\usepackage{booktabs}
\usepackage{array}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

% better lists
\usepackage[inline]{enumitem}
\setlist{nosep}

% authors
\usepackage{authblk}
\title{Which Scenario Should We Design For? Insights from House Elevation for the Multiple PDF Problem}
\author[1]{James Doss-Gollin}
\author[2,3,4]{Klaus Keller}
\affil[1]{Department of Civil and Environmental Engineering, Rice University}
\affil[2]{Department of Geosciences, the Pennsylvania State University}
\affil[3]{Earth and Environmental Systems Institute, the Pennsylvania State University}
\affil[4]{Thayer School of Engineering, Dartmouth College}
\renewcommand*{\Affilfont}{\normalsize\normalfont}

% ACRONYMS
\usepackage[acronym,nopostdot,nonumberlist,shortcuts,]{glossaries}
\newacronym{bdt}{BDT}{Bayesian decision theory}
\newacronym{bfe}{BFE}{base flood elevation}
\newacronym{fema}{FEMA}{the Federal Emergency Management Agency}
\newacronym{gcm}{GCM}{general circulation model}
\newacronym[]{gev}{GEV}{generalized extreme value}
\newacronym{lsl}{LSL}{local mean sea level}
\newacronym{mcmc}{MCMC}{Markov Chain Monte Carlo}
\newacronym{pdf}{PDF}{probability density function}
\newacronym{rcp}{RCP}{representative concentration pathway}
\newacronym{slr}{SLR}{relative sea level rise}
\newacronym{rdm}{RDM}{robust decision making}
\newacronym[plural=SOWs,descriptionplural=states of the world]{sow}{SOW}{state of the world}
\newacronym{tc}{TC}{tropical cyclone}

\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}

\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\newcommand{\usd}[1]{\SI{#1}[\$]{}}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}

% use biblatex
\usepackage{csquotes}
\usepackage[
  backend=biber,
  doi=true,
  url=false,
  isbn=false,
  style=authoryear-comp,
  natbib=true,
  backref=false,
  maxbibnames=10,
  maxcitenames=2,
  uniquename=false,
  uniquelist=false,
  sorting=nyt,
]{biblatex}
\renewbibmacro{in:}{}
\AtEveryBibitem{\clearfield{month}\clearfield{day}\clearfield{pages}\clearlist{language}}
\addbibresource{library.bib}

% load this last
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}

% up to 1250 words
\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}
    TBD
\end{abstract}

\clearpage

\section{Introduction}\label{sec:introduction}

Since infrastructure built today is likely to persist for several decades, infrastructure designs must take into account the stresses that a project is likely to face far into the future.
On the one hand, overly optimistic assumptions of future risk can lead to unacceptably high risk of failure in the future if additional investments are not made.
For example, a failure to prepare Texas's interconnected natural gas, electricity, and water systems for temperatures that had been observed in the past 30 years caused cascading failures with disparate impacts \citep{doss-gollin_txtreme:2021,busby_cascadingrisks:2021}.
On the other, unless opportunity costs are negligible, designing for a too-pessimistic scenario can contribute to maladptation by limiting resource availability in the future.
For example, \ldots\james{add an example of a maladaptive megaproject -- look to sustainability science or the arguments in \citet{ansar_bigisfragile:2017}, something by Flyvbjerg, \etc}

TODO: set up flood risk management as a context where over- or under-design could be relevant, then raise house elevation in the coastal zone as a specific example.
Federal guidance for this decision relies on standards (the $T$-year flood) rather than risk-based design.
Be sure to note that this has parallels to lots of other engineering design guidance. Then note the work of \citet{zarekarizi_suboptimal:2020} or \citet{xian_elevation:2017}, who show that a risk-based approach may improve significantly on a standards-based approach.

Additionally, the floodplain maps used for decision supportare silent on projected future changes and neglect the deep and dynamic uncertainties surrounding the projected hazards\ldots
Both risk-based design and standards-based approaches are complicated by nonstationarity.
On the one hand, nonstationarity of flood hazard is already evident in many places, especially in coastal areas where \gls{slr} is clear.
On the other, efforts to find objective methods for quantifying nonstationarity, eg of precipitation \citep{atlas14_texas:2018} or flood frequency \citep{bulletin17c:2019}, have struggled to find a satisfactory approach because factors like future emissions and localized climate response create deep uncertainties that resist objective quantification \citep[\eg,][]{keller_management:2021}.

This suggests that either nonstationarity should be ignored, or subjective approaches for designing for nonstationarity should be adapted.
Given that physical theories consistent with the observational record and models robustly project changes to climate hazard under warning, ignoring nonstationarity seems suboptimal.
Thus decision-makers need some subjective way to choose which possible futures to design for.

In this paper, we use a didactic case study of house elevation in Norfolk, VA as a lens through which to shed light on the following research questions.
\begin{description}
    \item[Q1] What are the potential benefits to homeowners of changing threshold-based guidance to risk-based guidance under nonstationary flood hazard?
    \item[Q2] How does risk-based guidance change under different assumptions of physical model structure and human emissions forcings?
    \item[Q3] How can deep uncertainties be transparently and consistently synthesized for decision analysis?
\end{description}
Specifically, we examine this problem through three lenses.

The paper is organized as follows.
In \cref{sec:case-study} we describe our case study in Norfolk, VA.
First, in \cref{sec:multiple-simulation} we analyze each simulation (\ie, realizations from these \glspl{pdf}) independently and illustrate .
Next, in \cref{sec:multiple-pdf} we consider the design problem as a set of ``multiple \glspl{pdf}'' and use these findings to motivate theoretical advances.
Then in \cref{sec:synthesizing} we present a probabilistic method for synthesizing simulations from multiple \glspl{pdf} and illustrate the advantages and potential pitfalls of this approach.
Finally in \cref{sec:conclusions} we briefly summarize our findings, discuss implications for engineering practice, and identify needs for future research.

\section{Case study}\label{sec:case-study}

We model a \emph{hypothetical} house in Norfolk, VA, where sea level rise drives nonstationary future flood hazard.

For interpretability, we focus on deep uncertainty in \gls{slr} and treat other model parameters as fixed quantities or as outputs from a single probabilsitic model, as shown in \cref{tab:uncertainties}.
We consider a single, one-time decision of whether to elevate a house, and if so by how much (\cref{fig:xlrm}).
We evaluate this decision for each of many possible simulations of \gls{slr} using a combined engineering-economic model based on that of \citet{zarekarizi_suboptimal:2020}.
In the remainder of this section we discuss data sources and the system model.

\begin{table}
    \centering
    \begin{tabular}{l l p{3in} l}
        \toprule
        Name           & Symbol           & Conceptual framework                                                                 & Section               \\
        \midrule
        \Gls{slr}      & $\overline{y}_t$ & Deeply uncertain: multiple methods considered                                        & \ref{sec:sea-level}   \\
        Storm surge    & $y'_t$           & Probabilistic: a single stationary \acrshort{gev} model                              & \ref{sec:storm-surge} \\
        Discount rate  & $1-\gamma$       & Fixed at 2\%                                                                         & \ref{sec:ead}         \\
        Depth-damage   & $D(h-y_t)$       & Deterministic: based on HAZUS model                                                  & \ref{sec:ead}         \\
        Elevation cost & $C(\Delta h)$    & Deterministic: a piecewise linear model following \citet{zarekarizi_suboptimal:2020} & \ref{sec:sow-metrics} \\
        Initial height & $h_0$            & Deterministic: \SI{0.5}{ft} above \gls{bfe} unless otherwise noted                   & TBD                   \\
        \bottomrule
    \end{tabular}
    \caption{Summary of uncertainties modeled and their treatment.}\label{tab:uncertainties}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=4in]{xlrm.pdf}
    \caption{
        Conceptual diagram of the decision problem.
        A \gls{sow} consists of a description of the uncertain factors (red).
        We model a single decision (yellow).
        For each combination of \gls{sow} and policy, the system model (gray) is used to calculate metrics describing the performance of the policy over the selected \gls{sow}.
    }\label{fig:xlrm}
\end{figure}

\subsection{Sea level rise}\label{sec:sea-level}

Deeply uncertain sea level rise is the focus of our model.
Our analysis considers the common case where a decision maker or analyst has access to an existing set of simulations.
Specifically, we use simulations published in \citet{ruckart_hazardous:2008} that use the BRICK model version 0.2 \citep{wong_brick0.2:2017}.

\Cref{fig:boxplots} shows the \glspl{pdf} of mean sea level in 2100 under each of the eight models considered: four emissions pathways (\gls{rcp} scenarios) times two models of ice sheet dynamics.
Variation between different \glspl{pdf} is dominated by the \gls{rcp} 8.5 scenario with fast ice sheet dynamics \citep[see][for a discussion of these dynamics]{wong_brick0.2:2017}.
This is also the \gls{pdf} with the greatest within-model variance.
\begin{figure}
    \centering
    \includegraphics[width=5in]{msl_boxplots.pdf}
    \caption{
        Sea level rise projections use the BRICK model \citep{wong_brick0.2:2017,ruckert_coastal:2019}.
        We consider \emph{eight time-dependent PDFs of sea level rise}: four \gls{rcp} scenarios $\times$ two parameterizations of ice sheet processes.
    }\label{fig:boxplots}
\end{figure}

For consistency with previous studies, we refer to one time series of \gls{slr} ($\overline{y}_t$) as \emph{a \acrfull{sow}}.

\subsection{Storm surges}\label{sec:storm-surge}

Neglecting hydrodynamics, we model annual maximum floods $y_t$ as the sum of sea level $\overline{y}_t$ and storm surges $y'_t$.

Data on storm surge comes from Sewells Point, VA (NOAA gauge 8638610).\james{cite}
Hourly recordings of water level are available from 1928 to the present; we use data from the period January 01 1928 to December 31 2021.
For each year we remove the annual mean then extract a single maximum; we refer the set of these maxima for all years as the time series of annual maximum storm surges.
This time series is shown in \cref{fig:observed-surges}(a).\james{This figure needs a bit of work: subpanel plots, equalizing axes, narrower time series, and no lines between the points}

\begin{figure}
    \centering
    \includegraphics[height=1.5in]{historic_surge.pdf}%
    \includegraphics[height=1.5in]{surge_return_period.png}
    \caption{
        Annual maximum storm surges (after subtracting mean sea level) at Sewells Point, VA.
        (a):
        time series of historic storms.
        Purple (orange) arrows denote notable tropical cyclones (nor'easters).
        (b):
        return periods.
        Dots indicate observed values; their $x$-value (``plotting position'') is calculated using the Weibull formula .
        Blue line shows a maximum likelihood GEV fit and the gray lines show Monte Carlo draws from the full posterior distribution of the Bayesian GEV fit.
    }
    \label{fig:observed-surges}
\end{figure}

We model future storm surge risk through a probabilistic lens.
To do this, we develop a stationary Bayesian model for storm surge:
\begin{equation}\label{eq:surge-model}
    y'_t \sim \text{GEV}\left(\mu, \sigma, \xi \right),
\end{equation}
where $y'_t$ is the storm surge (above sea level) in year $t$.

When GEV distributions are fit to time series data without strong prior information, the resulting uncertainties can be unrealistically large.\james{cite}
Thus, information from other locations, time periods, and/or from physical principles may help to reduce uncertainties \citep{Merz:2008bl,Merz:2008eh}.
To constrain estimates, we add weakly informative priors.
However, rather than adding prior information over the parameters themselves, we follow \citet{coles_evd:1996} and apply a prior over extreme quantiles of the distribution.
Specifically, we apply LogNormal priors over the 90th, 99th, and 99.9th percentiles of the distribution (\ie, the 10, 100, and 1000 year return period events).
These values were chosen to represent plausible physical ranges (the assumed 90\% confidence interval for the 10 year event is XY years, for the 100 year event is XZ years, and for the 1000 year event ZY years) and are plotted in supporting figure SX.\james{return to this sentence}

To evaluate a single \gls{sow}, we integrate over $J$ samples from the posterior distribution (of \cref{eq:surge-model}).

\subsection{Expected annual damage}\label{sec:ead}

There are two components of the system model (``relationships'' in \cref{fig:xlrm}).
The first is a fragility model that estimates the expected flood loss for a particular year, given the elevation of the house and the mean sea level for that year.
Specifically, expected annual damages in year $t$ is defined as the expected damages in a given year, conditional on the house's height relative to the gauge after elevation ($h = h_0 + \Delta h$):
\begin{equation}
    \textrm{EAD}_t = \mathbb{E}[d_t | h] = \int_{y_t} p(y_t) D(h - y_t) dy_t,
\end{equation}
where $y_t = \overline{y}_t + y'_t$ is the annual maximum flood, relative to the gauge, in year $t$, and $D(h - y_t)$ is the damage as a function of depth relative to the house.
Following \citet{zarekarizi_suboptimal:2020} we use the HAZUS deterministic depth-damage relationship to parameterize $D(h-y_t)$ (see fig. XYZ(c)).\james{Cite HAZUS}

The expected annual damages is sometimes calculated by assuming analytically tractable functional forms for the depth-damage relationship and for the  distribution of hazard \citep[\eg][]{vandantzig_dike:1956}.
However, the convolution of the HAZUS depth-damage equation with the GEV posterior does not have a tractable analytic solution so we instead estimate it through a Monte Carlo method:
\begin{enumerate}
    \item For $k=1, \ldots, K$:
          \begin{enumerate}
              \item sample a draw from the posterior distribution of flood hazard to get $\left\{ \mu_k, \sigma_k, \xi_k \right\}$
              \item simulate a single storm surge from the stationary distribution (equation XYZ) and add the mean sea level to get total flood depth
              \item calculate the flood damages for this draw by plugging the annual maximum flood depth ($h - y_t$) into  the deterministic HAZUS depth-damage relationship
              \item store this as the $k$th damage
          \end{enumerate}
    \item Estimate expected annual damages as the sample mean of the $K$ estimates
\end{enumerate}

However, evaluating expected annual damages for each of $N$ simulations of sea level rise, each of $J$ draw from the posterior distribution of storm surge, and each of $T$ time steps requires $N \times J \times T \times K$ simulations.
Since $K$ must be large in order to accurately approximate the integral, this requires incurring a heavy computational cost.
However, noticing that this function dependsonly on the elevation of the house relative to \gls{slr}, we develop a simuple emulator for expected annual damages given this difference: $\hat{D}(h - \overline{y})$.
To do this, we  precompute expected annual damage for all height differences in \SI{0.1}{ft} increments from -XY to YX ft and fit a piecewise linear interpolation to this data.\james{check increements, piecewise linear}
We use $K=\num{1e6}$ samples to fit this emulator for each of the NUMBER increments.
This model is shown in supplemental figure SX.\james{supplemental figure}
Once this interpolation has been precomputed, calculating expected annual damage for a particular year only requires evaluating a piecewise linear function.

\subsection{Lifetime expected damages}\label{sec:led}

The second component of the system model converts a time series of annual expected damages (\cref{sec:ead}) into lifetime expected damages, which we define as the upfront cost plus the discounted sum of expected annual damages
\begin{equation}\label{eq:led}
    \textrm{LED} = \sum_{t=1}^T \gamma^{t-1} \mathbb{E}[d_t | \Delta h],
\end{equation}
where $\gamma$ is the 1 minus the discount rate.
For the didactic purposes of this study we neglect uncertainty in the discount rate \citep{zarekarizi_suboptimal:2020,arrow_discount:2013} and use a fixed discount rate of 2, thus $\gamma = 0.98$.
Neglecting uncertainty in house lifetime, we model flood damages from 2022 to 2071, thus $T=50$.\james{This has not been the case thus far}
We calculate lifetime expected damages \emph{for each \gls{sow} separately}.

\subsection{Assessing performance}\label{sec:sow-metrics}

To assess the performance of a given decision for a specific \gls{sow}, we calculate three metrics for each \gls{sow}.
\begin{enumerate}
    \item The first metric considered is ``upfront cost,'' which reflects the cost of elevating a house. Following \citet{zarekarizi_suboptimal:2020}, we use estimates of construction cost from the Coastal Louisiana Risk Assessment \citep{fischbach_clara:2012}. We normalize this cost by house value.\james{Normalize!}
    \item The second metric is the lifetime expected damages (\cref{eq:led})
    \item The third metric is lifetime expected costs, which is the sum of lifetime expected damages and upfront costs
    \item The third metric is the expected benefit-to-cost ratio.  We define this ratio in terms of the ratio of construction cost to damages averted relative to not elevating at all:
          \begin{equation}
              \mathrm{BCR}(\Delta h) = \begin{cases} \Delta h = 0 & 1 \\ \Delta h \neq 0 & \frac{\mathbb{E}[d_t | \Delta h = 0] - \mathbb{E}[d_t | \Delta h = \Delta h]}{C(\Delta h)}\end{cases}
          \end{equation}
\end{enumerate}

\section{Exploring possible states of the world}\label{sec:multiple-simulation}

The literature on decision making under deep uncertainty \citep[see][]{marchau:2019} offers some insight as to how to identify decisions that perform well over many plausible \acrfullpl{sow}.

The first that we consider is exploratory modeling.
In general, we consider $J$ simulated \glspl{sow} $\qty{s_1, \ldots, S_J}$ and $I$ possible decisions $\qty{x_1, \ldots, x_I}$.
As described in the previous section, these $J$ \glspl{sow} represent time series of mean sea level and the $I$ decisions represent how high to elevate the house.
For each combination of these, we calculate the vector of metrics described in \cref{sec:sow-metrics}: $u_{ij} = f(x_i, s_j)$ using the system model $f$, which includes storm surge (\cref{sec:storm-surge}), damage (\cref{sec:ead,sec:led}), and calculation of additional metrics (\cref{sec:sow-metrics}).
This framework is illustrated in \cref{fig:flowchart} inside the box labeled Section 3\klaus{Any suggestions for how I can draw these regions better?}; we return in \cref{sec:multiple-pdf,sec:synthesizing} to add additional steps to the model.

\begin{figure}
    \centering
    \includegraphics[width=4in]{bayes-rdm.pdf}
    \caption{
        Outline of the generally applicable decision theoretic framework developed.
        We begin in \cref{sec:multiple-simulation} by presenting results using exploratory modeling, \ie nodes  (a), (b), and (c).
        In \cref{sec:multiple-pdf} we discuss nodes (d) and (e) to understand scenario-conditional probability distributions.
        Finally in \cref{sec:synthesizing} we add nodes (f) and (g) to synthesize across multiple \glspl{pdf}.
    }\label{fig:flowchart}
\end{figure}

Analysis of these $u_{ij}$ sheds light on the question: ``how well does a given decision perform under different possible futures?''
\Cref{fig:scenario-map} shows each possible future as a single dot for each of several possible values for $\Delta h$.
On the $y$ axis is expected lifetime costs (see \cref{sec:sow-metrics}).
On the $x$ axis is mean sea level in 2100, which is an interpretable 1-dimensional projection of the \glspl{sow}.
Although our model for expected discounted total costs is deterministic, points with similar $x$-values may have different $y$-values because their sea levels before 2100 can differ.

\begin{figure}
    \includegraphics[width=\textwidth]{scenario_maps.png}
    \caption{
        Scenario maps: one dot represents one \gls{sow}.
    }\label{fig:scenario-map}
\end{figure}

\Cref{fig:scenario-map} shows that the best outcome is when $\Delta h = 0$ and when \gls{slr} is negligible.
However, the worst outcome is also when $\Delta h = 0$ and when \gls{slr} is substantial.
As $\Delta h$ increases, the construction cost increases but the variability between different \glspl{sow} decreases, up to a point.
\emph{No decision dominates} any other, including $\Delta h = 0$, in the formal sense of being optimal over all \glspl{sow}.

\section{The multiple PDF problem}\label{sec:multiple-pdf}

Whereas the analysis in \cref{sec:multiple-simulation} shed light on how decisions perform under different \glspl{sow}, the metrics calculated represent the performance of a given decision-\gls{sow} combination, and \emph{do not shed light on the performance of the decisions themselves}.

An alternative framework is to interpret the \glspl{sow} as draws from each of eight \glspl{pdf}.
This has the approach of a probabilistic interpretation: \emph{conditional on a particular model} (\gls{rcp} scenario and ice sheet model structure), we can view corresponding \glspl{sow} as IID draws from the distribution of outcomes.

Some general notation is shown in \cref{fig:flowchart} (additional terms are added in \cref{sec:synthesizing}).
We have $K=8$ models of sea level rise, which we refer to as $\qty{\mathcal{M}_1, \ldots, \mathcal{M}_K}$.


\subsection{Metrics}

This allows us to reason formally about the \gls{pdf} of outcomes for each candidate decision, conditional on a model.
We can use these \glspl{pdf} of outcomes to calculate metrics describing the performance of decisions.



Any optimal strategy or Pareto frontier is conditional upon an (explicit or implicit) model of future outcomes.

\begin{figure}
    \centering
    \includegraphics[width=2.5in]{tradeoffs_height_totalcost_byscenario.pdf}~
    \includegraphics[width=2.5in]{tradeoffs_height_iqr_byscenario.pdf}
    \caption{
        Tradeoffs between (L) construction cost and expected lifetime costs or (R) construction cost and the uncertainty of future lifetime costs depend on the PDF selected.
    }
    \label{fig:tradeoffs}
\end{figure}

Including additional models \citep[\eg,][]{kopp_evolving:2017,deconto_antarctica:2016,ruckert_coastal:2019} \emph{would compound this challenge}.

Weights assigned to PDFs or simulations, \textbf{including implicit uniform weights}, should be communicated transparently to facilitate critique and improvement.\james{TODO: create a figure showing expected outcomes if you design for scenario $x$ and get scenario $y$}

\section{Synthesizing across PDFs for decision relevance}\label{sec:synthesizing}

\subsection{Methods}

While comparing results across many PDFs is a valuable exploratory exercise (see, \eg, \cref{fig:tradeoffs}), providing end users (\eg, households practicing engineers, and planning departments of local governments) with many PDFs may lead to confusion, inconsistency, and potentially liability \citep[see][]{schneider_dangerous:2001,schneider_scenarios:2002}.

Our approach to synthesizing PDFs is designed for the common case of assessing decisions using simulations (SOWs) from each of $K$ PDFs.
A system model $(f)$ quantifies the performance $(u)$ of a decision ($x$) under a single SOW ($s$).
To synthesize, we weight PDFs using an approach based on Bayesian Model Averaging \citep{wong_surge:2018,Yao:2018bu}.

\subsection{Findings}

\begin{figure}
    \centering
    \includegraphics[width=6in]{inference_weights.pdf}
    \caption{
        The weights assigned to each PDF (from \cref{fig:boxplots}) under each prior considered.
    }
\end{figure}
``Pessimistic'' (``Optimistic'') prior heavily weights \gls{rcp} 8.5 (2.6), which is unlikely given current policies \citep{hausfather_scenarios:2020}.

\begin{figure}
    \centering
    \includegraphics[width=3in]{lsl_priors.pdf}~
    \includegraphics[width=3in]{tradeoffs_height_totalcost_byprior.pdf}
    \caption{
        The prior model averaging approach (\cref{fig:flowchart}), uses a probability distribution representing subjective belief to average insights across each PDF available.
        Alternative priors can provide robustness checks.
    }
\end{figure}

\section{Conclusions}\label{sec:conclusions}

To summarize:
\begin{itemize}
    \item Natural extensions to Bayesian updating
    \item Belief $p(s)$ can't be ``right''  \citep{gelman_workflow:2020,gelman_philosophy:2013} or ``neutral'' \citep{quinn_exploratory:2020}
    \item Instead, we can ask \textbf{what assumptions are different priors consistent with?}
\end{itemize}

This didactic example illustrates a need for better synthesis and communication of deep uncertainties for decision making.
\textbf{\scshape let's collaborate on:}
\begin{itemize}
    \item More complex models to capture more relevant metrics
    \item Better models of nonstationary hazards
    \item Interacting, sequential decisions
\end{itemize}

\clearpage

Most fundamentally, we are thinking about robustness a bit differently than usual.
The typical way that the DMDU literature thinks about it is robustness to a \gls{sow}.
We are thinking about robustness to different modeling structures and assumptions.
This has links to the ``rival framings'' literature.
Given a subjective prior $p(s)$, I can compute expectations of the decision metrics $m$; if there are a lot of decisions, I can start to sketch out the tradeoff curves as shown in figures XXXX (in \cref{sec:results}).

\begin{enumerate}
    \item Links to DMDU literature
          \begin{enumerate}
              \item We should also cite \citet{quinn_exploratory:2020}, which makes the point that it matters how you sample \glspl{sow}.
              \item Exploratory modeling can demonstrate the existence of particular outcomes, generate hypotheses, build qualitative insight, and identify scenarios worthy of further study \citep[see][]{bankes:1993}.
              \item You can do scenario discovery
              \item We are in effect calculating robustness metrics \citep{mcphail_robustness:2019,herman:2015}
          \end{enumerate}
    \item Philosophy of Bayesian stats / decision making
          \begin{enumerate}
              \item The observation that many of these mechanisms cannot be represented by a single objective \gls{pdf} has motivated many criticisms of the application of \gls{bdt} to planning problems.
                    Yet \gls{bdt} was conceived as a calculus for reasoning rather than for identifying objective truth; \citeauthor{definetti_probability:1972} often said that ``probability does not exist'' \citeyear{definetti_probability:1972}.
                    \citet{savage:1954} and \citet{ramsey_probability:2016}, among others, also viewed probability as ``subjective,'' representing the state of belief of the decision-maker.
                    The famous phrase ``all models are wrong, but some are useful'' \citep[generally attributed to][]{box:1976} also suggests that probability distributions and predictions ought to be viewed subjectively.
                    More recent discussions of Bayesian philosophy \citep{jaynes_probability:2003,McElreath:2016vu,Gelman:2014tc,bernardo_bayesian:1994} also emphasize a philosophical view of probability as a language with which to reason about the unknown rather than a statement of objective truth \citep[see][for a thorough discussion of Bayesian philosophy]{gelman_philosophy:2013}.\klaus{Am I reading right that you say ``maybve have this in intro to motivate the analysis?''}
                    Although the true data generating process is not known and inference should not be represented as objective truth, probability gives a transparent and consistent language for reasoning about uncertainty.
                    Since modeling assumptions cannot overcome epistemic uncertainty, even with better models and more data, we draw from the literature on statistical model selection in the $\mathcal{M}$-open setting, which provides a theoretical background for choosing between models when the true data generating process is not among the models considered \citep[see][]{Piironen:2017eh}.
                    Often, combining inferences from multiple models is more effective than seeking a single ``best'' model \citep{Yao:2018bu}.
                    More fundamentally, this literature emphasizes the importance of iteratively building models, simulating the consequences of those models, and critiquing them \citep{gelman_workflow:2020}.
                    Since, by definition, models are not ``true,'' this iterative workflow aims to identify models that are useful and promote a dialog amongst stakeholders \citep{gelman_philosophy:2013}\footnote{this paragraph is copied from my dissertation, needs to be re-worked!}
          \end{enumerate}
    \item There are interesting links to Bayesian model averaging
          \begin{enumerate}
              \item Lots of examples to cite if we want
              \item I'm partial to something like stacking \citep{Yao:2018bu}
              \item The main difference is we are using only the prior to average the models!
          \end{enumerate}
    \item Limitations of the case study
          \begin{enumerate}
              \item Objectives: real people might care about uncertainty (risk aversion), probability of experiencing flooding at all (disruptions are hard to quantify), usable space created under the house, and more
              \item More uncertainty (damage functions, cost of construction, lifespan, discount rate, \etc)
              \item Better data on cost of elevation and depth-damage
              \item Robustness: ptimize separately for different kinds of house structures and locations
              \item Timing
          \end{enumerate}
    \item Limitations of the method
          \begin{enumerate}
              \item Our prior $p(s)$ is limited -- we are just using \gls{lsl} in 2100 but we could be looking at more parameters of it including rate of change, \etc
              \item We neglect true ignorance \citep{knight_risk:1921}.
              \item The real world is in a state of ``unknown unknowns'' \citep[level 5 as defined in][fig.~1]{walker_deep:2013} so trying to represent \emph{all} uncertainty is futile; we must make subjective modeling choices and assumptions about what is most important
          \end{enumerate}
    \item The benefits of this approach:
          \begin{enumerate}
              \item A goal is to make -- intrinsically flawed and subjective -- modeling choices as transparent as possible. Since we can't be right, we should make it as easy as possible for others to understand and critique our assumptions\james{Focus on this!}
              \item Because of how we weight realizations of the future, you can apply a different weighting function and immediately assess performance. Thus qualitative and quantitative sensitivity analyses are simple.
              \item Of course the choice of $p(\mathcal{Z})$ is subjective, but so is every other aspect of how we frame the problem. Modeling is intrinsically subjective. The idea of embracing subjective choices -- and making them explicit and open to critique -- draws heavily from philosophies of iterative workflow in statistics \citep{box:1976,gelman_workflow:2020,gelman_philosophy:2013}.
              \item This approach can be used in a multiobjective context and is an alternative way to measure robustness in \gls{rdm} and MORDM
          \end{enumerate}
    \item This approach can be used in many other contexts
          \begin{enumerate}
              \item Direct
                    \begin{enumerate}
                        \item Stormwater management \citep{sharma_rcp:2021,lopez-cantu:2018}
                        \item Levee heightening \citep{garner_slrise:2018,oddo_coastal:2017,vandantzig_dike:1956}
                    \end{enumerate}
              \item Indirect
                    \begin{enumerate}
                        \item Lots of economic models like social cost of carbon or effect of policy on metrics of interest make assumptions about deep uncertainties!
                        \item Model structure uncertainties
                    \end{enumerate}
          \end{enumerate}
\end{enumerate}.

\section{Conclusions}

\begin{enumerate}
    \item We started with the problem of ``which scenario should we design to?'' We then devised a cop-out: design to a weighted combination of all the scenarios. (So how should we wight the scenarios? Come up with a model $p(\Omega)$ that tells you.)
    \item There is no objective way to choose $p(\Omega)$ because we don't have data on the future!
    \item We can't get around from choosing $p(\Omega)$ implicitly\footnote{I'm not sure we have demonstrated this rigorously, or whether we want to argue it, but I think there's a way to frame this point that is valid and helpful}
    \item Ultimately, engineering design needs to consider not only how to weight different future scenarios, but also which metrics are important, how risk aversion should be addressed, \etc.
    \item We used a didactic case study to present a toolkit (decision framework, melding priors, prior over return levels) for making transparent and critique-friendly assessments.
\end{enumerate}

\section*{End Matter}

\subsection*{Acknowledgements}

\begin{enumerate}
    \item MARISA\james{details}
    \item PSIRC\james{details}
    \item Rice University (for startup)
    \item Tor Erlend Fjelde for helpful advice on implementing the GEV prior using \texttt{Turing.jl}.
    \item For many journals, a code and data statement will go here.
\end{enumerate}

\subsection*{Author Contributions}

See journal requirements and format

\subsection*{Supplementary Materials}

\section{Old material}

\clearpage
\newpage

Federal guidance for this decision relies on standards (the $T$-year flood) rather than risk-based design.
Be sure to note that this has parallels to lots of other engineering design guidance.
Then note the work of \citet{zarekarizi_suboptimal:2020} and \citet{xian_elevation:2017} who show that a risk-based approach may improve significantly on a standards-based approach.\james{Need to addd some citations of ``risk-based design''}

Additionally, the floodplain maps used for decision supportare silent on projected future changes and neglect the deep and dynamic uncertainties surrounding the projected hazards\ldots
Both risk-based design and standards-based approaches are complicated by nonstationarity.
On the one hand, nonstationarity of flood hazard is already evident in many places, especially in coastal areas where \gls{slr} is clear.
On the other, efforts to find objective methods for quantifying nonstationarity, \eg of precipitation \citep{atlas14_texas:2018} or flood frequency \citep{bulletin17c:2019}, have struggled to find a satisfactory approach because factors like future emissions and localized climate response create deep uncertainties that resist objective quantification \citep[see][]{keller_management:2021}.

This suggests that either nonstationarity should be ignored, or subjective approaches for designing for nonstationarity should be adapted.
Given that physical theories consistent with the observational record and models robustly project changes to climate hazard under warning,

Many methods for decision making under deep uncertainty first evaluate candidate decisions over many plausible states of the world, then aggregate performance over these possible futures.
Through a didactic case study of determining how high to elevate a single home in Norfolk, VA, we demonstrate that the common practice of weighting all scenarios equally in this aggregation creates a tension between (a) fully exploring the parameter space, including unlikely regions, and (b) accurately representing available scientific knowledge.
We introduce an approach to bridge this divide through a computationally efficient method that weights each state of the world based on a probability distribution over possible futures.
Since the distribution of deeply uncertain variables is necessarily subjective, we turn to frameworks for model critique from applied Bayesian statistics to compare and contrast different modeling assumptions.
This approach can help to improve decision-making by facilitating iterative and collaborative model improvement.

While classical utility- or regret-based frameworks offer a structure for weighing these risks, they are not designed to handle uncertainties that are \emph{deep} in the sense that stakeholders may not agree upon their probabilities \citep{lempert_complex:2002} or \emph{irreducible} in the sense that there is not an objective truth that better models and data can identify \citep{DossGollin:2019}.\footnote{Unless we believe everything is pre-determined, in which case why bother with climate adaptation\ldots}
Examples of such uncertainties include how much greenhouse gasses humans will emit in the future or whether the U.S. federal government will continue to subsidize flood insurance for homeowners.\james{Add some references}
In response, a wide range of fields have developed scenario-based methods\ldots.\klaus{Which arguments in \citet{savage:1954} did you think should be cited here?}
This leads to the ``Multiple PDF Problem'' \citep{sharma_rcp:2021}\ldots.
A key question, then, facing the designers, builders, regulators, owners, and users of built facilities is \emph{to which scenario should infrastructure be designed?}

One example is building codes and guidelines\ldots.
Prior studies have found that floodproofing and building-scale vulnerability reduction measures, including house elevation, can effectively reduce local flood damages in many contexts \citep{demoel_reducing:2014,deruig_building:2020,kreibich_building:2005,slotter_floodproofing:2020,Rozer:2016dn,mobley_mitigation:2020,aerts_cost:2018}.
Official guidance for homeowners, notably from \gls{fema}, recommends elevating to the \gls{bfe} (typically the \SI{100}{year} flood) plus a freeboard \citep{fema_retrofitting:2014,asce_24-14:2015,fema_retrofitting:2014} but recent suggests scope for improvement.
For example, \citet{xian_elevation:2017} used a cost-benefit analysis to demonstrate that tailoring recommendations to the inital elevation of a structure can reduce expected costs.
\citet{zarekarizi_suboptimal:2020} show that neglecting uncertainty in discount rate, house lifespan, flood risk, and depth-damage curves can lead to maladaptation.
Yet these studies are silent on the question of how nonstationary flood hazard should factor into this decision.

Nonstationarity is particularly relevant in coastal communities where \gls{slr} is expected to drive future flood risk.
As a didactic case study, we consider the problem of how high a homeowner should elevate their structure to mitigate future flood losses, illustrated in \cref{fig:house-sketch},\james{Need to revisit this picture} given stochastic flood peaks and deeply uncertain \gls{slr}.
\Gls{slr} is driven mainly by physical processes like (\ldots, West Antarctic Ice Sheet, \ldots\klaus{You wrote something here that I can't read -- I see something that looks like ICM. I assume Tony and Kelsey papers are relevant as well?}) \citep{kopp_probabilistic:2014,kopp_evolving:2017}.
Two key uncertainties identified by previous studies are
\begin{enumerate*}[label=(\roman*)]
    \item which emissions pathway (\eg, \gls{rcp} scenario) will we experience and
    \item how will polar ice sheets respond to warming \citep{wong_brick0.2:2017,ruckert_coastal:2019,wong_nola:2017,deconto_antarctica:2016}.
\end{enumerate*}
Simulations of \gls{slr} are available to decision makers grouped by \gls{rcp} scenario and model structure.
For a given combination of \gls{rcp} scenario and model, results are typically presented and interpreted as probabilistic, but this is not the same as giving decision-makers a single \gls{pdf}.
This is the ``Multiple \gls{pdf} Problem'' described in, \eg, \citet{sharma_rcp:2021}.
Thus, \emph{which scenario/\gls{pdf} should we design for?}

Lacking a crystal ball capable of telling us precisely which \gls{pdf} we need to design for, we have to consider several as plausible.
Any decision will perform differently in different scenarios.
There is no objective answer to this question.
Since we can't be ojectively correct, we might as well be transparent and make our assumptions explicit (and subject to critique) rather than implicit.

Households elevate their homes to manage flood risks, but regulations and guidance are silent on key questions \citep{zarekarizi_suboptimal:2020,xian_elevation:2017}.
\begin{description}
    \item[Q1] How to adapt guidance to building characteristics or household preferences?
    \item[Q2] How does nonstationary hazard change guidance?
    \item[Q3] Which model of nonstationary hazard to use?
\end{description}
These challenges also affect other adaptation plans and engineering designs (\cref{fig:stormwater-bridge}).

\begin{enumerate}
    \item Supplementary figures and analysis online
    \item Link to live repository on GitHub
    \item Link to code DOI on Zenodo
\end{enumerate}

\printbibliography
\end{document}
