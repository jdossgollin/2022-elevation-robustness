\documentclass[11pt]{article}

% Figures
\usepackage{graphicx}
\usepackage[list=true]{subcaption}
\graphicspath{{../../plots/}{../../tikz/}{../../img/}}
\usepackage[section]{placeins} % require floats to appear in the section they are defined

% fonts and appearance
\usepackage{amsmath, amsfonts, physics, siunitx, nicefrac}
\usepackage[american]{babel} 
\usepackage[T1]{fontenc} % improved font encoding
\usepackage[ttscale=0.8]{libertine}
\usepackage{fontawesome5}
\usepackage[format=plain, textfont=it]{caption}

% page size and margins
\usepackage{geometry}
\geometry{letterpaper,top=1in, bottom=1in, left=1in, right=2in}

% footer
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage[en-US]{datetime2}
\fancyhf{}
\fancyhead[L]{CONFIDENTIAL DRAFT by J. Doss-Gollin \& K. Keller}
\fancyhead[R]{\DTMnow}
\fancyfoot[R]{page~\thepage~of~\pageref{LastPage}}
\pagestyle{fancy}

% TO DO NOTES
\usepackage{xcolor} % list of colors at https://en.wikibooks.org/wiki/LaTeX/Colors
\definecolor{giallo}{HTML}{F0BC42} % https://teamcolorcodes.com/a-s-roma-color-codes/
\definecolor{rosso}{HTML}{8E1F2F}
\definecolor{grigio}{HTML}{CACACC}
\definecolor{nero}{HTML}{000000}
\usepackage[textsize=scriptsize]{todonotes}
\setlength{\marginparwidth}{1.5in}
\newcommand{\james}[1]{\todo[color=giallo, textcolor=nero]{\textbf{ATTN James:~}#1}} % if desired create a custom command for each author
\newcommand{\klaus}[1]{\todo[color=rosso, textcolor=grigio]{\textbf{ATTN Klaus:~}#1}}

% better tables
\usepackage{booktabs}
\usepackage{array}
\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

% better lists
\usepackage[inline]{enumitem}
\setlist{nosep}

% authors
\usepackage{authblk}
\title{A subjective Bayesian framework for synthesizing deep uncertainties in climate risk management}
\author[1]{James Doss-Gollin}
\author[2]{Klaus Keller}
\affil[1]{Department of Civil and Environmental Engineering, Rice University}
\affil[2]{Thayer School of Engineering, Dartmouth College}
\renewcommand*{\Affilfont}{\normalsize\normalfont}

% ACRONYMS
\usepackage[acronym,nopostdot,nonumberlist,shortcuts,]{glossaries}
\newacronym{abc}{ABC}{approximate Bayesian computation}
\newacronym{bdt}{BDT}{Bayesian decision theory}
\newacronym{bfe}{BFE}{base flood elevation}
\newacronym[]{cdf}{CDF}{cumulative distribution function}
\newacronym{fema}{FEMA}{the Federal Emergency Management Agency}
\newacronym{gcm}{GCM}{general circulation model}
\newacronym[]{gev}{GEV}{generalized extreme value}
\newacronym{iid}{IID}{independent and identically distributed}
\newacronym{ipcc}{IPCC}{International Panel on Climate Change}
\newacronym{mcmc}{MCMC}{Markov Chain Monte Carlo}
\newacronym{msl}{MSL}{mean relative sea level}
\newacronym{noaa}{NOAA}{the National Oceanic and Atmospheric Administration}
\newacronym{pdf}{PDF}{probability density function}
\newacronym{rcp}{RCP}{representative concentration pathway}
\newacronym{rdm}{RDM}{robust decision making}
\newacronym{slr}{SLR}{sea level rise}
\newacronym{ssp}{SSP}{shared socio-economic pathway}
\newacronym[plural=SOWs,descriptionplural=states of the world]{sow}{SOW}{state of the world}
\newacronym{tc}{TC}{tropical cyclone}

\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}

\usepackage{xspace}
\makeatletter
\DeclareRobustCommand\onedot{\futurelet\@let@token\@onedot}
\def\@onedot{\ifx\@let@token.\else.\null\fi\xspace}
\newcommand{\usd}[1]{\SI{#1}[\$]{}}
\def\eg{\emph{e.g}\onedot} \def\Eg{\emph{E.g}\onedot}
\def\ie{\emph{i.e}\onedot} \def\Ie{\emph{I.e}\onedot}
\def\etc{\emph{etc}\onedot} \def\vs{\emph{vs}\onedot}

% use biblatex
\usepackage{csquotes}
\usepackage[
  backend=biber,
  doi=true,
  url=false,
  isbn=false,
  style=authoryear-comp,
  natbib=true,
  backref=false,
  maxbibnames=10,
  maxcitenames=2,
  uniquename=false,
  uniquelist=false,
  sorting=nyt,
  giveninits=true,
]{biblatex}
\renewbibmacro{in:}{}
\AtEveryBibitem{\clearfield{month}\clearfield{day}\clearfield{pages}\clearlist{language}}
\addbibresource{library.bib}

% load this last
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}

% up to 1250 words
\begin{document}
\maketitle
\thispagestyle{empty}

\begin{abstract}
    Climate risk management involves comparing candidate decisions under deep and dynamic uncertainties\ldots
\end{abstract}

Key points
\begin{enumerate}
    \item
\end{enumerate}

\section{Introduction}\label{sec:introduction}

Flood risk management relies on probabilistic estimates of flood hazard:
\begin{enumerate}
    \item Formal flood risk management in the United States is implemented through a mix of floodplain management policies, design standards, and insurance programs.
    \item Local governments can build infrastructure to reduce flood risk; they often prioritize these by considering a reduction in damages from a design storm (\eg, the 100 year storm) \citep{hcfcd_prioritization:2019}
    \item There is a role for industry organizations that set standards (\eg, reliability) that local governments often incorporate into building codes \citep{asce_infrastructure_climate:2021}
    \item The National Flood Insurance Program was designed as a partnership between the federal government and local communities; communities can voluntarily join the program by adopting a floodplain ordinance based on the most up-to-date flood hazard maps provided by FEMA. At a minimum, communities that choose to join must require that new development and substantially improved or damaged properties in high hazard areas be built at or above the level of a flood with a return period of one in 100 years (i.e., the 100-year flood). \citep{kousky_voucher:2014}
    \item Beyond flood insurance and disaster recovery there has been a federal retreat from flood risk management leaving local governments and individuals with most of the responsibility for flood risk management: ``the federal government recognizes its risks as a major owner and operator of infrastructure, facilities, \ldots, and insurer of properties and crops \ldots leaving states, localities, and individual agencies to make their own plans'' \citep{shi_transformative:2021}
\end{enumerate}
Future flood hazard is deeply uncertain
\begin{enumerate}
    \item ``Along the U.S. coastline, public infrastructure and \$1 trillion in national wealth held in coastal real estate are threatened by rising sea levels, higher storm surges, and the ongoing increase in high tide flooding'' \citep{reidmiller_reportinbrief:2018}
    \item Nonstationarity also affects inland and pluvial flooding \citep{Milly:2008dg} and derives from a combination of climate change, land use change, and local river modifications \citep{Merz:2014gf}
    \item Federal guidelines for flood risk management that directly inform policy-making recognize the presence of nonstationarity but have failed to identify objectively valid approaches for modeling nonstationarity, \citep{Montanari:2014hl,Serinaldi:2015bq}
    \item For coastal flooding, \gls{fema} guidance suggests using up to date estimates of \gls{msl} \citep{fema_slr:2016}, but no projections, and stationary estimates of storm surge \citep{fema_ffa:2016}, although the analyst at each location has considerable flexibility
    \item Ditto extreme rainfall \citep{atlas14_texas:2018} or floods \citep{bulletin17c:2019}
    \item This is because where historic flood risk can be constrained by estimates, future flood risk depends on intrinsically unpredictable human decisions like greenhouse gas emissions, and is therefore deeply uncertain \citep{keller_management:2021}
\end{enumerate}
Methods for decision making under deep uncertainty emphasize exploring many possible futures
\begin{enumerate}
    \item deep uncertainty describes ``known unknowns'' where there is not a single objectively true model \citep{walker_deep:2013,Walker:2013gi,lempert_complex:2002}
    \item This is related to equifinality where different models fit the historical record equally well but diverge as one looks into the future \citep{beven_equifinality:2006,DossGollin:2019}
    \item Applying classical optimization methods to problems with deep uncertainty can yield highly fragile solutions in the sense that guidance might change substantially if a different plausible model were used
    \item \citet{bankes:1993} draws a distinction between exploratory and consolidative modeling
    \item Decision scaling \citep{Steinschneider:2015kk}
    \item global sensitivity analysis \citep{saltelli_sensitivity:2010,herman_salib:2017,sobol_sensitivity:2001} feeds large scenario ensembles into a black box model, then analyzes outputs to assess the relative importance of each input (conditional on an assumed joint distribution over inputs)
    \item MORDM \citep{kasprzyk:2013} combines exploratory modeling (exploring performance over many scenarios) and policy search (using optimization tools)  \citep{kasprzyk:2013,kasprzyk_denovo:2012,hadka_mordm:2015}. Typically ``robustness metric'' is used \citep{herman:2015,mcphail_robustness:2019}
    \item RDM-like methods have been applied to coastal infrastructure planning by sampling \glspl{sow} from a \gls{pdf} of \gls{slr}, then identifying decisions that meet some predetermined satisficing criteria \citep{mcphail_robustness:2019} over a large fraction of possible futures \citep{sriver_sealevel:2018,garner_slrise:2018,lempert_slr:2012}
    \item For a more complete review see \citet{marchau:2019}
    \item Crucially, these approaches all emphasize exploring performance over many scenarios, then describing this performance into some set of metrics.
\end{enumerate}
Some critiques
\begin{enumerate}
    \item There are always subjective  modeling decisions, but sometimes they are opaque which makes it hard to communicate them to the end user
    \item The range you choose is also subjective! \citep{schneider_dangerous:2001,schneider_scenarios:2002}
    \item \citet{schneider_scenarios:2002} criticized the reluctance of the \gls{ipcc} special report on emission scenarios \citep{nakicenovic_scenarios:2000} to assign probabilities to scenarios, writing ``if we in the scientific assessment business do not offer some explicit notions of the likelihood of projected events, then the users of our products -- policy analysts and policy makers -- must guess what we think these likelihood estimates are.'' Different end users will, in general, have different preferences that motivate different design choices given the same information, it is unlikely that these end users have expertise in global emissions pathways, climate sensitivity, or ice sheet dynamics.
    \item Technical detail: joint probability distributions over high dimensional spaces are confusing. Bayesian analysis teaches us that posterior distributions of parameters are often highly correlated. If we neglect that by sampling over the marginal distribution of each parameter separately, we may be sampling almost entirely from regions of the parameter space that are not consistent with our data \citep[see][for a detailed illustration]{carpenter_dimensionality:2017}!
    \item Some outcomes of deeply uncertain processes are more likely than others \citep{hausfather_scenarios:2020,ho_scenarios:2019,srikrishnan_probabilistic:2022} yet this is generally not acknowledged in, \eg, \gls{ipcc} analysis
    \item Technical detail: sampling over ranges is not invariant to parameterization: if I sample $\theta \sim \mathrm{Uniform}(a, b)$ and then resample $\log theta \sim \mathrm{Uniform}(\log a, \log b)$, I'll get different samples and thus different estimates
\end{enumerate}
The question of whether probability is a useful tool for decision making under deep uncertainty depends on how one interprets probability; we argue that a subjective Bayesian interpretation of probability is useful for many DMDU problems
\begin{enumerate}
    \item We are inspired by methods and philosophy of Bayesian model choice in the ``$\mathcal{M}$-open'' case where there is no true model to identify
    \item Many modern discussions of Bayesian philosophy \citep{jaynes_probability:2003,McElreath:2016vu,Gelman:2014tc,bernardo_bayesian:1994} also emphasize a philosophical view of probability as a language with which to reason about the unknown rather than a statement of objective truth \citep[see][for a thorough discussion of Bayesian philosophy]{gelman_philosophy:2013}. This is in many ways closer to the exploratory modeling proposed by \citet{bankes:1993} than the top-down, deterministic, fragile central planning approaches that it (and \cite{rittel:1973} and others) critiqued
    \item \gls{bdt} was conceived as a calculus for reasoning rather than for identifying objective truth; \citeauthor{definetti_probability:1972} often said that ``probability does not exist'' \citeyear{definetti_probability:1972}. \citet{savage:1954} and \citet{ramsey_probability:2016}, among others, also viewed probability as ``subjective,'' representing the state of belief of the decision-maker.
    \item We know that choosing a single ``best'' model under-estimates uncertainty so inferences need to be combined across models \citep[\eg, as in stacking;][]{Yao:2018bu}
    \item In the absence of data that would tell us how accurate each model is via some likelihood function, we have to use the prior.
    \item The famous phrase ``all models are wrong, but some are useful'' \citep[generally attributed to][]{box:1976} also suggests that probability distributions and predictions ought to be viewed subjectively.
    \item Although the true data generating process is not known and inference should not be represented as objective truth, probability gives a transparent and consistent language for reasoning about uncertainty.
    \item Since modeling assumptions cannot overcome epistemic uncertainty, even with better models and more data, we draw from the literature on statistical model selection in the $\mathcal{M}$-open setting, which provides a theoretical background for choosing between models when the true data generating process is not among the models considered \citep[see][]{Piironen:2017eh}.
    \item Often, combining inferences from multiple models is more effective than seeking a single ``best'' model \citep{Yao:2018bu}. More fundamentally, this literature emphasizes the importance of iteratively building models, simulating the consequences of those models, and critiquing them \citep{gelman_workflow:2020}.
    \item Since, by definition, models are not ``true,'' this iterative workflow \citep{gelman_workflow:2020} aims to identify models that are useful and promote a dialog amongst stakeholders \citep{gelman_philosophy:2013}
\end{enumerate}
Our aim is to illustrate how methods for model selection in the $\mathcal{M}$-open case can improve decision making under deep uncertainty.
\begin{enumerate}
    \item We want to combine multiple models in a way that is transparent, and that can be changed with low computational cost (cheap robustness checks).
    \item We are particularly motivated by problems for which simulations of some key uncertainties (particularly climate-driven ones but also population, economic growth, etc) already exist. We assume only that there is no coupling between the system model and these simulations, so they can be treated as fully exogenous.
\end{enumerate}
We illustrate our approach with a didactic case study\ldots.
\begin{enumerate}
    \item House elevation in the coastal zone as a specific example of a problem where both over- or under-investment could be a problem
    \item Federal guidance uses both standards (floodplain, \gls{bfe}) and cost-benefit analysis (does it pass test?)
    \item Be sure to note that this has parallels to lots of other engineering design guidance.
    \item \citet{xian_elevation:2017}: one size does not fit all
    \item \citet{zarekarizi_suboptimal:2020}: neglecting uncertainty leads to bad outcomes
    \item Prior studies have found that floodproofing and building-scale vulnerability reduction measures, including house elevation, can effectively reduce local flood damages in many contexts \citep{demoel_reducing:2014,deruig_building:2020,kreibich_building:2005,slotter_floodproofing:2020,Rozer:2016dn,mobley_mitigation:2020,aerts_cost:2018}.
    \item Official guidance for homeowners, notably from \gls{fema}, recommends elevating to the \gls{bfe} (typically the \SI{100}{year} flood) plus a freeboard \citep{fema_retrofitting:2014,asce_24-14:2015,fema_retrofitting:2014} but recent suggests scope for improvement.
    \item For example, \citet{xian_elevation:2017} uses a cost-benefit analysis to demonstrate that tailoring recommendations to the initial elevation of a structure can reduce expected costs.
          \citet{zarekarizi_suboptimal:2020} show that neglecting uncertainty in discount rate, house lifespan, flood risk, and depth-damage curves can lead to maladaptation.
    \item Yet these studies are silent on the question of how nonstationary flood hazard should factor into this decision.
    \item FEMA's Hazard Mitigation Grant Program, authorized under Section 404 of the 1988 Stafford Act, which awards grants after a disaster. Under the Stafford Act, projects funded by HMGP, including buyouts, must be shown to be cost-effective; typcally, up-front costs are compared to avoided losses \citep{bendor_buyouts:2020}
\end{enumerate}
Our primary research question is ``how can deep uncertainties be transparently and consistently synthesized for decision analysis?''
We proceed as follows\ldots

\section{Decision analytic framework}\label{sec:analysis}

In this section we outline our framework for decision analysis under deep uncertainty, maintaining a high level of generality.
In the next we discuss application to the house elevation case study.

Following \cref{fig:flowchart}, consider using $J$ \glspl{sow} $\vb{s} = \qty{s_1, s_2, \ldots, s_J}$ (box b in \cref{fig:flowchart}) to evaluate $I$ discrete decisions $\vb{x} = \qty{x_1, x_2, \ldots, x_I}$ (box a in \cref{fig:flowchart}).
For each scenario $s_j \in \vb{s}$ and decision $x_i \in \vb{x}$ we use a system model $f$ to calculate a set of metrics describing the performance of decision $x_i$ on \gls{sow} $s_j$, which we denote $u_{ij} = f(x_i, s_j)$ (box c in \cref{fig:flowchart}).
While we assume here that the \glspl{sow} and decisions are known and discrete, this approach could be  coupled to a policy search model that proposes candidate decisions $x_i$ from some decision space $\mathcal{X}$.\klaus{Is there better mathematical notation for panel (b)?}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{bayes-rdm.pdf}
    \caption{
        Outline of the proposed decision analytic framework.
    }\label{fig:flowchart}
\end{figure}

\subsection{Explore}\label{sec:analysis-explore}

A first analytical step is to use the model in an ``exploratory'' mode.
Exploratory modeling avoids making explicit assumption  about the likelihood different \glspl{sow} and instead seeks to generate new knowledge through existence proofs, hypothesis generation, and vulnerability identification \citep{bankes:1993}.
Common applications include ``discovering'' particularly relevant scenarios  \citep{lamontagne_discovery:2018,groves_scenarios:2007} and mapping a system's response to decisions and \glspl{sow} \citep{Brown:2012kb,Poff:2015jn,Steinschneider:2015kk,sriver_sealevel:2018}.
Although exploratory modeling seeks to avoid explicit assumptions about the likelihood of different futures, subjective modeling decisions including the choice of system model, the set of candidate decisions, the criteria used to assess outcomes, and the choice of how to sample \glspl{sow} \citep[see][]{quinn_exploratory:2020} strongly influence results.

\subsection{Condition}\label{sec:analysis-condition}

Although exploratory modeling is a useful framework for understanding systems, there are many questions that it cannot answer.
For example, answering questions like ``what is the 95th percentile of some metric under decision $x$'' or ``which decision minimizes expected damages'' or ``what is the probability of failing to satisfy a critical metric''  requires estimating the expectation of some function $\pi$, which has the form
\begin{equation*}
    \mathbb{E}\qty[f(s | x_i)] = \int_{s \in \Omega} p(s) \pi(s | x_i) \dd{s}.
\end{equation*}
Because expectations are defined relative to a probability distribution, \emph{answering these questions necessitates defining a probability distribution over \glspl{sow}}.
The general intractability of these integrals motivates the use of Monte Carlo methods by sampling across many \glspl{sow} \citep{Betancourt:2017vd}.

In boxes (d) and (e) of \cref{fig:flowchart} we add this concept to our decision analytic framework.
Given some model $M_k$ specifying a probability over \gls{sow} $p(\Omega)$, we sample $\vb{s} \sim p(\Omega | M)$.
Since the \glspl{sow} are drawn \gls{iid} from $M_k$, the set of outcomes $u_{i, j}$ can be interpreted as \gls{iid} draws from the conditional distribution over outcomes, $p(u | x_i, M_k)$.

Many decision analytic tools used in climate risk management can be interpreted through this lens.
A common application (to which we return in our case study) is when using probabilistic models with deeply uncertain boundary conditions.
For example, \ldots\james{Add a water resources and a flood example}
In this case, $M$ describes both the choice of model and the choice of boundary condition (\eg, choice of physical model and the \gls{rcp} scenario).
As we shall illustrate in \cref{sec:results-conditional}, this analysis models uncertainty \emph{within models} probabilistically, but models uncertainty \emph{between models} only as a.\ldots\klaus{I'm stuck, can you help me finish this sentence?}

Another application is to studies that generate an ensemble of \glspl{sow} by sampling parameters across fixed ranges.
For example, \citet{lempert_slr:2012} sample parameters describing the rate of \gls{slr} across a range of values to inform coastal adaptation.
Similarly, \citet{trindade_waterpathways:2020} checks the performance of candidate decisions against an ensemble of synthetic time series of streamflow, water demand, and other parameters by sampling parameters that transform the available data over a plausible range \citep[\ie, calculating robustness metrics; see][for details]{mcphail_robustness:2019,herman:2015}.
Since sampling over a range is equivalent to sampling from a Uniform distribution, this method makes an implicit assumption that $M$ is the product of $L$ independent Uniform distributions (one for each parameter) over the plausible ranges $\qty(\theta_{\ell,\min}, \theta_{\ell,\max}) \forall \ell \in \qty{1, \ldots, L}$:
\begin{equation}
    p(\Omega | M) = \prod_{\ell=1}^L \mathrm{Uniform} \qty(\theta_{\ell,\min}, \theta_{\ell,\max}).
\end{equation}
If this product of independent Uniform distributions truly represents a well-calibrated belief about the joint probability distribution over \glspl{sow} $p(\Omega)$ then it should be used, but analysts should be clear that, like it or not, they are making probabilistic assumptions, and should apply methods for model critique and validation \citep[see][for some directions]{gelman_workflow:2020}.

Our primary concern is not that subjective assumptions about the likelihood of different futures are wrong -- this is, almost surely, inevitable -- but that when these assumptions are opaque and presented without critique or validation they may lead to sub-optimal decisions.

\subsection{Synthesize}\label{sec:analysis-synthesize}

If the ensemble of \glspl{sow} available does not represent our subjective belief about the plausibility of future conditions, then some corrective measure should be taken.
One possibility is to discard the $\vb{s}$ and resample from some ``true'' distribution $p_\mathrm{belief}$.
In practice this is impractical because simulating $\vb{s}$ may rely on complicated models that are computationally expensive to re-run.
Consequently, climate risk management often uses the outputs of structured simulations and analyses \citep[\eg, from the Coupled Model Intercomparison Project's simulations;][]{meehl_cmip:2000}; it is neither practical nor feasible for local decision-makers to re-run climate models every time they want to sample \glspl{sow} from a different distribution.

Instead of attempting to re-sample $\vb{s}$, we instead consider re-weighting.
To do this we first project the \glspl{sow} $\vb{s} \in \Omega$ onto a low-dimensional representation, which we denote $\qty{\psi_1, \psi_2, \ldots, \psi_J} \in \Psi$.
This allows us to specify $p_\mathrm{belief}(\Psi)$ instead of $p_\mathrm{belief}(\Omega)$.
We then calculate a probabilistic weight $w_j \in \qty[0, 1]$ for each \gls{sow} $s_j$ such that $\sum_{j=1}^J w_j \psi_j$ approximates $p_\mathrm{belief}(\Psi)$.\klaus{Really we are setting the weights so that $\sum_{j=1}^J w_j \pi(s_j)$ approximates $\int_{s} p(s) \pi(s) \dd{s}$ -- is this too hand-wavey?}
We present here the 1D case.
Without loss of generality, assume the $\psi_j$ to be sorted from least to greatest so that $\psi_{j-1} \leq \psi_j$, ($j \neq 1$).
Defining $F_\mathrm{belief}(s)$ to be the cumulative distribution corresponding to $p_\mathrm{belief}$, we calculate weights as illustrated in \cref{fig:grid-sketch}:
\begin{equation}\label{eq:weights}
    w_j = \begin{cases}
        F_\mathrm{belief}\qty(\frac{1}{2}\qty[\psi_1 + \psi_2])                                                                     & j = 1     \\
        F_\mathrm{belief}\qty(\frac{1}{2}\qty[\psi_{j} + \psi_{j+1}]) - F_\mathrm{belief}\qty(\frac{1}{2}\qty[\psi_{j-1} + \psi_j]) & 1 < j < J \\
        1 - F_\mathrm{belief}\qty(\frac{1}{2}\qty[\psi_{J-1} + \psi_J])                                                             & j = J.
    \end{cases}
\end{equation}
For higher dimensional projections, this equation can be extended by partitioning the parameter space into a Voronoi diagram, and then integrating the \gls{pdf} $p_\mathrm{belief}$ over the boundaries of each region.
From a practical perspective, many simulation models round outputs, meaning that there is a possibility of having $\psi_j$ that are equal; in this case it may be helpful to add a small amount of noise to the $\psi_j$ before computing the weights.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{grid-sketch}
    \caption{
        Schematic of \gls{sow} weighting scheme defined in \cref{eq:weights} for $J=4$.
        This method is illustrated for a hypothetical target distribution (orange line) and four samples $\psi_1, \psi_2, \psi_3, \psi_4$ (blue dots).
        As shown in \cref{eq:weights}, the weights $w_j$ (vertical blue lines) are calculated based on the \gls{cdf} of the target distribution at the halfway points $\frac{1}{2}\qty[\psi_i+\psi_{i+1}]$ (vertical dashed lines).
    }\label{fig:grid-sketch}
\end{figure}

The aim of this re-weighting framework is to integrate an ensemble of \glspl{sow} used for exploratory modeling into formal decision analysis.
As in \cref{sec:analysis-condition} we must condition on a model, but whereas that analysis conditions upon deep uncertainties, this approach synthesizes them.
It is crucially important to understand that stakeholders and experts will not, in general, agree on $p_\mathrm{belief}$ because its uncertainty is epistemic -- that is, there is no ``true'' value of $p_\mathrm{belief}$ that could be estimated if only we had better models and data.
However, reasoning using subjective probability allows\ldots\james{Finish sentence}

\section{Case study}\label{sec:case-study}

We model a one-time decision of whether to elevate a house, and if so by how much (\cref{fig:xlrm}), for a \emph{hypothetical} house in Norfolk, VA.
For interpretability, we focus on deep uncertainty in \gls{msl} and treat other model parameters as more shallow uncertainties as shown in \cref{tab:uncertainties}.
To use the notation developed in the previous section, we consider the following.
\begin{enumerate}
    \item The decision vector $\vb{x}$ is a scalar describing how high to elevate a house ($\Delta h$); we consider $\Delta h = \qty{\SI{0}{ft}, \SI{0.25}{ft}, \ldots, \SI{12}{ft}}$ f
    \item The \glspl{sow} describe annual time series of \gls{msl} over the $T$ year house lifetime ($T=71$ generally; see \cref{tab:uncertainties}) so $\vb{s} \in \mathbb{R}^T$
    \item The system model $f$ quantifies up-front costs and lifetime expected damages, given a decision $x_i$ and \gls{sow} $s_j$, by integrating economic and engineering damage models over a probability distribution for storm surge.
\end{enumerate}
In the remainder of this section we describe data sources and treatment of \gls{slr} (\cref{sec:case-slr}), storm surge (\cref{sec:case-surge}), damages and metrics (\cref{sec:case-metrics}), and finally the subjective priors $p_\mathrm{belief}$ used to apply the re-weighting method described in \cref{sec:analysis-synthesize} to this case study (\cref{sec:case-priors}).

\begin{table}
    \centering
    \caption{
        Summary of parameters, their notation, and how their uncertainty is represented.
    }\label{tab:uncertainties}
    \small
    \begin{tabular}{l l p{3.5in}}
        \toprule
        Name             & Symbol            & Uncertainty                                                                          \\
        \midrule
        \Gls{msl}        & $\overline{y}(t)$ & Deeply uncertain: four physical models $\times$ four \acrshort{rcp} scenarios        \\
        Storm surge      & $y'(t)$           & Probabilistic: Bayesian inference on a stationary \acrshort{gev} model               \\
        Ann-max flood    & $y(t)$            & Deterministic: $y(t)=\overline{y}(t)+y'(t)$                                          \\
        Discount rate    & $1-\gamma$        & Fixed at 2.5\%                                                                       \\
        Depth-damage     & $D(h-y)$          & Deterministic: based on HAZUS model                                                  \\
        Elevation cost   & $C(\Delta h)$     & Deterministic: a piecewise linear model following \citet{zarekarizi_suboptimal:2020} \\
        Initial height   & $h_0$             & Deterministic: \SI{1}{ft} below the \gls{bfe} unless otherwise noted                 \\
        House floor area &                   & Deterministic: \SI{1500}{ft^2} unless otherwise noted                                \\
        Structural value &                   & Deterministic: \usd{200000} unless otherwise noted                                   \\
        House lifespan   & $T$               & Deterministic: 71 years unless otherwise noted                                       \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{xlrm.pdf}
    \caption{
        Conceptual diagram of the decision problem.
        A \gls{sow} consists of a description of the uncertain factors (red).
        We model a problem with a single lever (yellow), which is how high to elevate a house ($\Delta h$).
        For each \acrshort{sow} and each value of $\Delta h$, the system model (blue) is used to calculate performance metrics (orange).
    }\label{fig:xlrm}
\end{figure}

\subsection{\Glsentrylong{slr}}\label{sec:case-slr}

We analyze simulations of \gls{msl} at Sewells Point, VA from four probabilistic physical models using data published in \citet{ruckert_coastal:2019}.
The four models considered are (i) the BRICK model (version 0.2) with slow (``BRICK Slow'') and (ii) fast (``BRICK Fast'') ice sheet dynamics \citep{wong_brick0.2:2017}, (iii) the \citet{kopp_probabilistic:2014} model (``K14''), and (iv) the \citet{deconto_antarctica:2016} model (``DP16'').
The \citet{kopp_probabilistic:2014} and \citet{deconto_antarctica:2016} models have a ten year time step, which we linearly interpolate onto a one year time step for consistency.
These four models represent physical processes, particularly of ice sheet dynamics, in different ways, leading to diverging sensitivity of \gls{msl} to forcing.
For a discussion of these methods, their similarities, their differences, and their consistency with other analyses see \citet{ruckert_coastal:2019}, \citet{kopp_evolving:2017}, and \citet{bamber_slrise:2019}.

Estimates of nonstationary \gls{msl} also depend on anthropogenic forcing, which is itself deeply uncertain.
To sample this uncertainty, we use simulations from each physical model under four \gls{rcp} scenarios, yielding sixteen time-varying distributions of \gls{msl}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{lsl-evolution}
    \caption{
        Projections of future mean sea level depend strongly on the choices of physical model and forcing.
        (A): 90\% confidence intervals for mean sea level at Sewells Point, VA as a function of time for a representative subset of three probabilistic models (out of sixteen).
        (B): probability distribution of \gls{msl} at Sewells Point, VA in the year 2100 for each probabilistic model considered.
    }\label{fig:lsl-evolution}
\end{figure}

The choices of physical model and \gls{rcp} scenario jointly determine future \gls{msl} $p(\overline{y}|t)$.
\Cref{fig:lsl-evolution}(a) shows the time-varying \glspl{pdf} of \gls{msl} for three representative models.
The divergence between the the best-case (blue) and worst-case (red) models is small in the early 21st century and increases rapidly thereafter.
\Cref{fig:lsl-evolution}(b) shows the \glspl{pdf} of mean sea level in 2100 (dashed vertical line in panel (a)) under each of the sixteen models considered.
We return in \cref{sec:results-conditional} to the problem that these multiple models poses to decision makers.

\subsection{Storm surges}\label{sec:case-surge}

We model annual maximum floods $y(t)$ as the sum of sea level $\overline{y}(t)$, described in the previous subsection, and annual maximum storm surges $y'(t)$.

Data on storm surge comes from Sewells Point, VA (gauge 8638610) from the NOAA tides and currents dataset,  freely available to the public at \url{https://tidesandcurrents.noaa.gov/waterlevels.html}.
Hourly recordings of water level are available from 1928 to the present; we use data from the period January 1, 1928 to December 31, 2021.
For each calendar year we first remove the annual mean, then calculate the maximum water level.
We refer this time series of annual maximum storm surges as $y'(t)$.

In the historical record, major surges have been driven by two different mechanisms: tropical cyclones and Nor'easters.
This time series of annual maxima is shown in \cref{fig:surge-obs-return}(a).
The largest recorded surge was the Chesapeake-Potomac hurricane of 1933, which caused a surge of over \SI{7}{ft} at this gauge, but other hurricanes and Nor'easters have caused surges above \SI{6}{ft}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-obs-return}
    \caption{
        Annual maximum storm surges (after subtracting mean sea level) at Sewells Point, VA.
        (A):
        time series of historic storms.
        Red (yellow) arrows denote notable tropical cyclones (Nor'easters).
        (B):
        return periods.
        Dots indicate observed values; their $x$-value is their plotting position using the Weibull formula $\nicefrac{r}{N+1}$.
        Gray lines show the 50, 80, and 95\% posterior confidence intervals from the Bayesian \gls{gev} fit (\cref{sec:storm-surge}).
    }\label{fig:surge-obs-return}
\end{figure}

We model future storm surge using a stationary \gls{gev} model:
\begin{equation}\label{eq:surge-model}
    y'(t) \sim \text{GEV}\qty(\mu, \sigma, \xi),
\end{equation}
where $y'(t)$ is the storm surge (above sea level) in year $t$ and a \gls{gev} distribution with location $\mu$, scale $\sigma$, and shape $\xi$ has the probability density function
\begin{equation*}
    f(x | \mu, \sigma, \xi)= \begin{cases}
        \frac{1}{\sigma} \qty[ 1 + \qty( \frac{x-\mu}{\sigma} ) \xi ]^{-1 / \xi - 1} \exp \qty{- \qty[1+\qty(\frac{x-\mu}{\sigma}) \xi ]^{-1 / \xi} }, & \xi \neq 0 \\
        \frac{1}{\sigma} \exp \qty{-\frac{x-\mu}{\sigma}} \exp \qty{ -\exp \qty[-\frac{x-\mu}{\sigma}]},                                               & \xi=0.
    \end{cases}
\end{equation*}
This model assumes stationarity, neglecting any potential time dependence.

Our approach to model building follows a principled workflow for model building and checking \citep[see][for details]{gelman_workflow:2020}.
One model choice, analogous to the choice of statistical distribution or the assumption of stationarity, is the choice of how to represent prior information.
We include two forms of prior information.
First, we constrain $\xi > 0$ to reflect knowledge about the support of $y'$:
\begin{equation*}
    \mathrm{supp}~ y' =
    \begin{cases}
        \xi < 0: & y' \in \qty(-\infty,~ \mu - \nicefrac{\sigma}{\xi}) \\
        \xi > 0: & y' \in \qty(\mu-\nicefrac{\sigma}{\xi},~ \infty).
    \end{cases}
\end{equation*}
Since storm surges cannot be negative, only the latter is physically defensible so we constrain $\xi > 0$.
Second, we add weakly informative priors.
Rather than applying prior information directly over the joint distribution of the priors $\qty{\mu,\sigma,\xi}$, we instead apply a prior over extreme quantiles of the distribution, as in \citet{coles_evd:1996}.
Specifically, we apply Normal (truncated at zero) priors over the 2, 10, 100, and 500 year return levels, with means \SIlist{4;6;10;15}{ft} and standard deviations \SIlist{1.5;1.75;2.25;2.75}{ft}, respectively.
These values were chosen to represent plausible physical ranges and are plotted in \cref{fig:surge-gev-priors}.

For inference, we draw \num{10000} samples from the posterior distribution $p(\mu,\sigma,\xi | y')$ using Hamiltonian Markov Chain Monte Carlo \citep{Betancourt:2017vd,hoffman_nuts:2011} implemented in the Turing package of the Julia programming language \citep{perkel_julia:2019,ge_turing:2018,tarek_dynamicppl:2020,besancon_distributions.jl:2021,bezanson_julia:2012}.
Diagnostics suggest (though cannot guarantee) convergence (see \cref{tab:surge-posterior-mcmc-diagnostics}).
We evaluate the model's fit using posterior predictive checks \citep[see][section 2.4 and references therein]{gelman_workflow:2020}.
Using the lag 1 and 2 partial autocorrelations, sample maximum, sample minimum, sample median, and Mann-Kendall test value as Bayesian test statistics, we found that draws from the posterior predictive distribution matched the observed test statistic credibly (\cref{fig:surge-test-statistics}) although panels (a) and (b) suggest the possibility of temporal structure not captured by our stationary \gls{iid} model.
Future efforts could represent this structure by conditioning the parameters of the distribution on relevant climate indices \citep[as in][]{wong_structural:2020,Farnham:2016tw,farnham_jetstream:2017}.

\Cref{fig:surge-obs-return}(b) shows the estimated return periods for these storm surges.
The return period of the data (dots) is shown using the Weibull (``empirical'') plotting position, $\nicefrac{r}{N + 1}$, where $N$ is the sample size and $r$ is the order of the $N$ observations ($r=1$ is the largest, $r=N$ is the smallest).
The good fit of the Bayesian fit (gray shading) to the data (points) suggest the modeling choices made are reasonable and sufficient for this didactic example.
A positive control test (\cref{fig:surge-synthetic-data-experiment}) also validates the model's ability to recover known parameter values.

\subsection{Damages and metrics}\label{sec:case-metrics}

The system model (``relationships'' in \cref{fig:xlrm}) is comprised of two key pieces.
The first is a fragility model that estimates the expected flood damages for a particular year (``expected annual damages''), given the elevation of the house and the mean sea level for that year.
The second converts model converts a time series of annual expected damages into lifetime expected damages.

We define expected annual damages in year $t$ as the expectation of the damage function with respect to storm surge.
This expectation depends on the house's height ($h = h_0 + \Delta h$) where $h_0$ is the initial height relative to the gauge and $\Delta h$ is the amount by which the house is elevated.
The expected annual damage is thus
\begin{equation}\label{eq:ead}
    \textrm{EAD}(t) = \mathbb{E}[D \qty(h-\overline{y}(t))] = \int_{y'} p(y') D \qty(h - \qty(\overline{y}(t) + y')) \dd{y'}
\end{equation}
where $D(h-y)$ is a deterministic function specifying damage as a function of flood depth (relative to the house).
Following \citet{zarekarizi_suboptimal:2020} we use the we use the Hazard U.S. (HAZUS) depth-damage curves provided by FEMA; this depth-damage relationship is shown in \cref{fig:cost-depth-damage}.
For comparison, \cref{fig:cost-depth-damage} also shows the ``Europa'' depth-damage relationship developed by the Joint Research Center of the European Commission's science and knowledge service \citep{huizinga_depthdamage:2016}.
Although \citet{zarekarizi_suboptimal:2020} demonstrate that the choice of fragility function is important for informing house elevation, we use only the HAZUS model for simplicity.

The expected annual damage is sometimes calculated by assuming analytically tractable functional forms for the depth-damage relationship and for the  distribution of hazard \citep[\eg][]{vandantzig_dike:1956}.
However, the convolution of the HAZUS depth-damage equation with the \gls{gev} posterior does not have a tractable analytic solution so we instead estimate it through a Monte Carlo method (see \cref{sec:alg-ead} for details).
Then, because the expectation in \cref{eq:ead} depends only on $h-\overline{y}(t)$, we calculate expected annual damages for a wide range of possible heights, then use this to train a computationally efficient a surrogate model (see \cref{sec:surrogate-ead}).

The second component of the system model converts a time series of $\mathrm{EAD}$ into lifetime expected damages, which we define as the up-front  discounted sum of expected annual damages:
\begin{equation}\label{eq:led}
    \textrm{LED} = \sum_{t=t_i}^{t_f} \gamma^{\qty(t-t_i)-1} \textrm{EAD}(t),
\end{equation}
where the discount rate is $1 - \gamma$, the initial time $t_i=2022$, and the end time $t_f = t_i + T$.
Although \citet{zarekarizi_suboptimal:2020} show that uncertainty in the discount rate is important for decision support, we use a fixed discount rate (see \cref{tab:uncertainties}) for the purposes of this study.
For a more theoretical discussion see \citet{arrow_discount:2013}.

To assess the performance of a given decision for a specific \gls{sow} (``Metrics'' in \cref{fig:xlrm}), we calculate the following metrics for each decision-\gls{sow} combination:
\begin{enumerate}
    \item ``Up-front cost'' is the cost of elevating a house. Following \citet{zarekarizi_suboptimal:2020}, we use estimates of construction cost from the Coastal Louisiana Risk Assessment \citep{fischbach_clara:2012}. We normalize this cost by house value. This cost curve is shown in \cref{fig:cost-up-front}.
    \item ``Lifetime expected damages'' is calculated following \cref{eq:led}.
    \item ``Expected lifetime costs'' is the sum of lifetime expected damages and up-front costs.\james{Make sure the figures all use this term}
\end{enumerate}

\subsection{Prior over \glsentrylong{slr}}\label{sec:case-priors}

We construct three probabilistic models for $p_\text{belief}(\psi)$, which represents the amount of \gls{slr} from 2022 to 2100.
We use a Gamma distribution for all three priors, parameterized as
\begin{equation*}
    f(x | \alpha, \theta) = \frac{x^{\alpha-1} e^{-x/\theta}}{\Gamma(\alpha) \theta^\alpha},
    \quad x > 0.
\end{equation*}
\Cref{tab:slr-priors} specifies these priors, as well as some quantiles of the distributions.
Their \glspl{pdf} are also plotted in \cref{fig:lsl-priors-weights}(A).

\begin{table}[h]
    \centering
    \caption{
        Subjective priors over \gls{slr} from 2022 to 2100, \ie $p_\text{belief}(\psi)$.
        The name of the distribution, the parameters of the Gamma distribution with shape $\alpha$ and scale $\theta$, and the 2.5, 25, 50, 75, and 97.5th percentiles.
    }\label{tab:slr-priors}
    \input{../../plots/lsl_priors.tex}
\end{table}

We developed these priors for didactic purposes, to illustrate a range of possible beliefs.
We can compare them, for example, with analysis published by \gls{noaa}, which project \SIlist{1.94;2.62;4.27;5.25;6.89}{ft} for the low, intermediate, low intermediate, intermediate high, and high scenarios, respectively \citep[table.~2.4]{sweet_slr:2022}.

Future work could develop improved, policy-relevant priors, either by combining probabilistic models for emissions \citep[as in][]{srikrishnan_probabilistic:2022}, climate sensitivity, and the \gls{slr}, or through inversion of expert opinion \citep[as in][]{fuller_inversion:2017}.
Future work could also consider more sophisticated priors, either by considering higher-dimensional $\psi$ or by modeling a joint prior over model inputs and model outputs through melding \citep{poole_melding:2000,sevcikova_melding:2007}.

\section{Results and discussion}\label{sec:results}

We illustrate our approach to synthesizing uncertainties by sequentially analyzing the house elevation problem through the lenses of exploratory modeling (\cref{sec:results-exploratory}), scenario-conditional analysis (\cref{sec:results-conditional}), and finally the proposed synthesis method (\cref{sec:results-synthesis}).

\subsection{Exploratory modeling}\label{sec:results-exploratory}

We begin by using our model in an ``exploratory'' mode with an aim of learning about interactions between system dynamics and decisions.

\begin{figure}
    \includegraphics[width=\textwidth]{scenario-map-slr-cost}
    \caption{
        Scenario maps show the dependence of expected lifetime cost (damages plus up-front cost) as a function of \gls{msl} in 2100 for several values of $\Delta h$ and $h_0$.
        Colors indicate the density of \glspl{sow}; the color of each grid box corresponds to the number of \glspl{sow} falling within that box.
        The lowest-cost outcomes occur when both exposure is low ($h_0$ is large and \gls{slr} is minimal) and the house is not elevated (no up-front cost).
        The highest-cost outcomes arise when exposure is high ($h_0$ is small and \gls{slr} is rapid) and investment is inadequate.
        In all cases, elevating the house reduces the variance in total lifetime cost.
        Values are sensitive to model constants; see \cref{tab:uncertainties}.
    }\label{fig:scenario-map-slr-cost}
\end{figure}

\Cref{fig:scenario-map-slr-cost} shows the dependence of expected lifetime costs (damages plus up-front costs; $y$-axis) as a function of  \gls{slr} over the house lifetime ($x$-axis), height increase ($\Delta h$; columns), and initial elevation ($h_0$; rows).
The best outcomes (yellow) arise when the house is not elevated ($\Delta h = 0$) and \gls{slr} is minimal (bottom left corners).
The worst outcomes (blue) arise when the house is elevated only slightly and \gls{slr} is rapid.
As $\Delta h$ increases, the best-case scenario becomes more expensive because up-front costs increase, but worst-case scenarios become less expensive because even if \gls{slr} is substantial, damages will be negligible.

This analysis answers ``what-if'' questions like ``given $h_0$ and $\Delta h$, what is the range of total costs a homeowner could face if \gls{slr} over the house lifetime is \SI{1}{ft} or \SI{10}{ft}.''
For some decision-makers, contextualizing this information against a few scenarios of \gls{slr} \citep[\eg, those of][]{sweet_slr:2022} may prove sufficient for decision making.
This would be analogous to decision scaling methods that plot climate model projections on a scenario map of water system performance as a function of changes in key climate variables \citep{Brown:2012kb,Steinschneider:2015kk}.
However, this analysis does not shed light on cost-benefit analyses or return periods, nor does it permit quantitative comparison against other possible decisions.

\subsection{Scenario-conditional optimization}\label{sec:results-conditional}
We now turn to the scenario-conditional analysis described in \cref{sec:analysis-condition}.
Whereas the exploratory analysis of the previous subsection interpreted each time series of future sea level as a sample from the space of possible futures, we can also interpret each \gls{sow} as a draws from one of the sixteen models of \gls{slr} shown in \cref{fig:lsl-evolution}.
As discussed in \cref{sec:analysis-condition}, this allows a probabilistic interpretation: conditional on a particular \gls{rcp} scenario and physical model, we can reason about the distribution of outcomes.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{tradeoffs-by-rcp}
    \caption{
        Each probabilistic model or scenario leads to a different estimate of the Pareto frontier.
        (A): trade-off between up-front cost (which is a monotonic function of height increase) and expected lifetime costs.
        (B): trade-off between up-front cost and lifetime expected damages (eq.~\ref{eq:led}).
        Light gray lines show estimates for all 16 models (four \gls{rcp} scenarios times four physical parameterizations) considered.
        Colored lines highlight three representative models for emphasis.
    }\label{fig:tradeoffs-by-rcp}
\end{figure}

This probabilistic interpretation allows us to compute, for example, expected values.
For example, \cref{fig:tradeoffs-by-rcp}(a) plots the expected total lifetime cost as a function of $\Delta h$ for the sixteen models considered (we highlight three representative models).
Similarly, \cref{fig:tradeoffs-by-rcp}(b) plots the lifetime expected damages as a function of $\Delta h$.
This figure shows that for small $\Delta h$, expected costs are low under optimistic models (\eg, \gls{rcp} 2.6 with slow ice sheet dynamics) and high under pessimistic models (\eg, \gls{rcp}8.5 with the DP16 model).
Consistent with \cref{fig:scenario-map-slr-cost}, the variability of lifetime costs decreases as $\Delta h$ increases.
Once $\Delta h$ reaches \SIrange[]{3}{7}{ft}, depending on the model considered, construction costs start to dominate flood losses, and thus higher values of $\Delta h$ increase average lifetime costs.

This approach is, in a sense, another form of exploratory modeling: instead of considering a very large ensemble of \glspl{sow}, we consider a much smaller set of probabilistic models.
This approach is attractive because it allows modelers to focus on their domain expertise (\eg, the response of ice sheets and global sea levels to a particular climate future).
However, conditioning simulations on a set of climate futures and physical models presents what we term ``the multiple \gls{pdf} problem'' because it leaves decision makers with many \glspl{pdf} to choose from.
The multiple \gls{pdf} problem has also been shown in other contexts.
For example, \citet{sharma_rcp:2021}\klaus{According to Earth's Future this is in fact 2021} modeled the reliability of stormwater infrastructure under different climate models and downscaling methods, finding diverging estimates of future rainfall hazard, even under a single \gls{rcp} scenario.\james{If we think of other examples let's put them here}
Although this scenario-conditional analysis is appropriate for understanding differences between models, its key limitation is that \emph{it places the burden for deciding which model to design for onto the end user.}

\subsection{Synthesizing deep uncertainties for decision analysis}\label{sec:results-synthesis}

To overcome the limitations of scenario-conditional analysis, in this subsection we illustrate how the re-weighting method described in \cref{sec:analysis-synthesize} can shed light on climate risk management under deep uncertainty.
We present results using each of the models for $p_\mathrm{belief}$ outlined in \cref{sec:case-priors}.
These three distributions are shown in \cref{fig:lsl-priors-weights}(A).

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{lsl-priors-weights}
    \caption{
        Subjective priors for local sea level.
        We develop two distributions (``subjective priors'') representing plausible probabilistic beliefs about \gls{msl} at Sewells Point, VA in 2100, relative to the present.
        The \glspl{pdf} of these subjective priors are shown in panel (A).
        In panels (B-C) we show the relationship between these subjective priors and the 16 probabilistic models (four \gls{rcp} scenarios and four physical representations) available.
        Specifically, (B-C) show the average weight given to each model by each of the subjective priors.
    }\label{fig:lsl-priors-weights}
\end{figure}

One application of this method is to diagnose which assumptions different $p_\text{belief}$ are consistent with.
\Cref{fig:lsl-priors-weights}(B-D) shows the average weight that each prior assigns to \glspl{sow} generated by each \gls{rcp} scenario and physical model.
For example, the rapid \gls{slr} scenario places most weight on the DP16 model, and particularly on \gls{rcp} 8.5 which is unlikely given current policy \citep{hausfather_scenarios:2020,srikrishnan_probabilistic:2022}.
Conversely, the slow \gls{slr} scenario places most weight on the BRICK models, particularly \gls{rcp} 2.6 (also unlikely given current policy) and \gls{rcp} 4.5.
The uncertain \gls{slr} scenario places approximately equal weight across models.


This method can also be used to calculate expectations, allowing us to revisit the trade-off diagrams of \cref{fig:tradeoffs-by-rcp}.
\Cref{fig:tradeoffs-by-prior} shows the total lifetime cost (panel A) and lifetime expected damages (panel B) under each model.
Notably, they give different guidance.
Under an assumption of rapid \gls{slr}, elevating the house by approximately \SI{6}{ft} saves approximately 25\% of the house value relative to not elevating.
Under an assumption of slow \gls{slr}, elevating the house by \SI{6}{ft} costs approximately 25\% of the house value relative to not elevating.
Under the intermediate / uncertain \gls{slr} assumption, the expected lifetime costs are similar for elevating or not elevating the house.
Under all assumptions, elevating by only a few feet is impractical because it involves paying the large fixed costs of elevation (\cref{fig:cost-up-front}) but offers relatively little flood reduction.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{tradeoffs-by-prior}
    \caption{
        As \cref{fig:tradeoffs-by-rcp}, but Pareto frontiers are shown for the full distribution of outcomes, for three models of $p_\text{belief}$ (colors).
        Thin gray lines show the trade-off curves for each \gls{rcp} scenario and model separately as in \cref{fig:tradeoffs-by-rcp}.
    }\label{fig:tradeoffs-by-prior}
\end{figure}

\section{Conclusions}\label{sec:conclusions}

Quick summary of what our motivation was
\begin{enumerate}
    \item We are motivated by the work of \citet{sriver_sealevel:2018} and \citet{lempert_slr:2012}, which points out that while for the particular case study at hand probabilistic and RDM analyses gave similar results (unsurprising since the probabilistic analysis used uniform priors over ranges), the motivation of RDM was an interactive,  problem-focused and iterative exploration with stakeholders. We are motivated to develop a method with more formal probabilistic interpretations that can also be used in this iterative framework.
    \item Explore every \gls{sow}
    \item Be transparent about how we are calculating trade-offs
    \item Work with existing physical model ensembles
    \item We want to be robust to different plausible beliefs rather than robust to different outcomes
\end{enumerate}
Quick summary of what we have done
\begin{enumerate}
    \item It matters a lot which scenario you design for
    \item Robustness metrics and robust decision analyses make implicit assumptions about the plausibility of future scenarios
\end{enumerate}
Limitations of the re-weighting
\begin{enumerate}
    \item Our prior $p(\Omega)$ is limited -- we are just using \gls{msl} in 2100 but we could be looking at more parameters of it including rate of change, \etc
    \item The real world is in a state of ``unknown unknowns'' \citep[level 5 as defined in][fig.~1]{walker_deep:2013} so trying to represent \emph{all} uncertainty is futile; we must make subjective modeling choices and assumptions about what is most important
    \item Normal distributions used in \citet{fuller_inversion:2017} were applied for their simplicity and interpretability, but we note that they may not represent a well-calibrated prior belief about \gls{slr}; we refer the reader to \citet{gelman_workflow:2020} for a discussion of prior predictive checks that could be applied.
    \item Perhaps discarding the edge \gls{ssp} scenarios is unhelpful;
\end{enumerate}
Limitations of the case study
\begin{enumerate}
    \item Objectives: real people might care about uncertainty (risk aversion), probability of experiencing flooding at all (disruptions are hard to quantify), usable space created under the house, and more
    \item More uncertainty (damage functions, cost of construction, lifespan, discount rate, \etc)
    \item Better data on cost of elevation and depth-damage
    \item Scalability: optimize separately for different kinds of house structures and locations
    \item Timing
    \item More physics -- we neglect hydrodynamics and tides
\end{enumerate}
Future research needs:
\begin{enumerate}
    \item This didactic example illustrates a need for better synthesis and communication of deep uncertainties for decision making.
    \item More complex models to capture more relevant metrics
    \item Better models of nonstationary hazards
    \item Interacting, sequential decisions
    \item Once a model is chosen, it will under-represent total uncertainty, since this depends on both with- and between-model variability. We hypothesize this will lead to under-valuation of adaptive measures.
    \item Ultimately, developing probabilistic scenarios of future emissions \citep[as in][]{srikrishnan_probabilistic:2022}, temperature, and sea level rise would be valuable.
\end{enumerate}
Another clarification about how to interpret the $p_\mathrm{belief}$
\begin{enumerate}
    \item As statistitian David Spiegelhalter discusses, this subjective belief should not be interpreted as a population distribution because there is no true population distribution that could be answered by collecting more data \citep{andorra_spiegelhalter:2021}.\james{Contextualize this}
    \item
\end{enumerate}
This approach can be applied to lots of important problems
\begin{enumerate}
    \item Stormwater management \citep{sharma_rcp:2021,lopez-cantu:2018}
    \item Levee heightening \citep{garner_slrise:2018,oddo_coastal:2017,vandantzig_dike:1956}
    \item Lots of economic models like social cost of carbon or effect of policy on metrics of interest make assumptions about deep uncertainties!\james{Let's emphasize this -- really important to policy analysis under deep uncertainty}
    \item Model structure uncertainties
    \item Local government revenues are sensitive to flood losses; this approach can be used to structure updates to building codes \citep{shi_municipal:2020}
\end{enumerate}
We call for researchers studying climate risk management to make their implicit probability distributions more transparent, and we call for coordinated national and international guidance to assist local governments and engineering bodies determine appropriate subjective beliefs.

\section*{End Matter}

\subsection*{Acknowledgements}

This work was supported by \acrfull{noaa} through the Mid-Atlantic Regional Integrated Sciences and Assessments (MARISA) program under NOAA grant NA16OAR4310179 and by the Penn State Center for Climate Risk Management.
JDG thanks Rice University for support.
KK thanks Dartmouth College for support.
The authors thank Tor Erlend Fjelde for helpful comments.

\subsection*{Code and data availability}

All code, including source code, is available under the GNU Public License (version 3) at \url{https://github.com/jdossgollin/2021-elevation-robustness}.
This code is written in the open source Julia programming language.

\subsection*{Author Contributions}

JDG and KK designed the research.
JDG wrote the codes and ran simulations.
JDG wrote the manuscript in consultation with KK.
JDG and KK revised the manuscript.

\printbibliography

\appendix
\newcommand{\hbAppendixPrefix}{S}
\renewcommand{\thefigure}{\hbAppendixPrefix\arabic{figure}}
\setcounter{figure}{0}
\renewcommand{\thetable}{\hbAppendixPrefix\arabic{table}}
\setcounter{table}{0}
\renewcommand{\theequation}{\hbAppendixPrefix\arabic{equation}}
\setcounter{equation}{0}

\newpage
\section{Supplemental methods}

\subsection{Algorithm to estimate expected damages}\label{sec:alg-ead}

\begin{enumerate}
    \item For $k=1, \ldots, K$:
          \begin{enumerate}
              \item sample a draw from the posterior distribution of flood hazard to get $\qty{ \mu_k, \sigma_k, \xi_k }$
              \item simulate a single storm surge from the stationary distribution (\cref{eq:surge-model}) and add the mean sea level to get total flood depth $y^\mathrm{sim}_k$
              \item calculate the flood damages for this draw by plugging the annual maximum flood depth ($h - y_k$) into  the deterministic HAZUS depth-damage relationship, storing this as the $k$th damage.
          \end{enumerate}
    \item Estimate expected annual damages as the sample mean of the $K$ estimates
\end{enumerate}

\subsection{Surrogate model for expected annual damages}\label{sec:surrogate-ead}
Evaluating expected annual damages for each of $N$ simulations of \gls{slr}, each of $J$ draw from the posterior distribution of storm surge, and each of $T$ time steps requires $N \times J \times T \times K$ simulations.
Since $K$ must be large in order to accurately approximate the integral, this requires incurring a heavy computational cost.
However, noticing that this function depends only on the elevation of the house relative to \gls{msl}, we develop a simple emulator for expected annual damages given this difference: $\hat{D}(h - \overline{y})$.
To do this, we  precompute expected annual damage for all height differences in \SI{0.25}{ft} increments from \SIrange{-30}{30}{ft} and fit a piecewise linear interpolation to this data.
We use $K=\num{1e6}$ samples to fit this emulator for each of the 241 increments.
This model is shown in \cref{fig:cost-expected-damage-emulator}.
Once this interpolation has been precomputed, calculating expected annual damage for a particular year only requires evaluating a piecewise linear function.

\section{Supplemental figures}

\begin{figure}
    \centering
    \includegraphics[width=4in]{cost-depth-damage}
    \caption{
        Depth-damage relationship.
        Following \citet{zarekarizi_suboptimal:2020}, we use the Hazard U.S. (HAZUS) depth-damage curves provided by FEMA.
        Since results are sensitive to choice of depth-damage equation, we illustrate (for comparison only) the ``Europa'' depth-damage relationship developed by the Joint Research Center (JRC) of the European Commission's science and knowledge service \citep{huizinga_depthdamage:2016}.
    }\label{fig:cost-depth-damage}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=4in]{cost-expected-damage-emulator}
    \caption{
        As discussed in \cref{sec:ead}, we model expected annual damages (eq.~\ref{eq:ead}) as a function of the house's elevation relative to \gls{msl}.
        Damages ($y$ axis) are shown as a percentage of house value.
    }\label{fig:cost-expected-damage-emulator}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=4in]{cost-up-front}
    \caption{
        Following \citet{zarekarizi_suboptimal:2020}, we model the cost of elevating a single-family house by interpolating estimates from the Coastal Louisiana Risk Assessment Model \citep{johnson_clara:2013}.
        According to this model, the unit cost of elevating a house by 3-7, 7-10, and 10-14 feet is \usd{82.50}, \usd{86.25}, and \usd{103.75} per square foot, respectively, with a \usd{20745} initial cost.
        Values are sensitive to house floor area and structural value; see \cref{tab:uncertainties}.
    }\label{fig:cost-up-front}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=4in]{surge-gev-priors}
    \caption{
        Prior distributions for annual maximum storm surge.
        Rather than apply a prior over model parameters directly, we apply a weakly informative prior over quantiles of the resulting distribution (that is, over a function of the model parameters) following \citet{coles_evd:1996}.
        See \cref{sec:storm-surge} for details.
        For the 2, 10, 100, and 500 year events we apply Normal distributions, truncated at zero, with means \SIlist{4;6;10;15}{ft} and standard deviations \SIlist{1.5;1.75;2.25;2.75}{ft}, respectively.
    }\label{fig:surge-gev-priors}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-prior-return}
    \caption{
        Surge prior
    }\label{fig:surge-prior-return}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-synthetic-data-experiment}
    \caption{
        Synthetic data experiment as a positive control test for the \gls{gev} model of storm surge.
        A synthetic record was sampled from a \gls{gev} distribution with location, scale, and shape parameters of 4, 0.5, and 0.15, respectively (dots).
        These samples were used to fit the Bayesian \gls{gev} model described in \cref{sec:storm-surge}; the gray shading indicates the 50, 80, and 95\% posterior confidence intervals.
        The blue line shows the true quantiles of the (known) \gls{gev} distribution.
        By random chance the sample maximum has a true return period of $\gg 250$ years, which increases the upper confidence interval of the estimated return probabilities, but the true value is nevertheless within the 50\% posterior confidence interval.
        This experiment yields similar results for alternative values of the known \gls{gev} distribution, and for different random seeds (not shown).
    }\label{fig:surge-synthetic-data-experiment}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-posterior-chains}
    \caption{
        \Gls{mcmc} plots for posterior draws from the storm surge model.
        We draw \num{10000} samples by running four chains of \num{3500} iterations each and discarding the first \num{1000}.
        The mixing of the chains is consistent with, though does not guarantee, convergence.
    }\label{fig:surge-posterior-chains}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-prior-chains}
    \caption{
        As \cref{fig:surge-posterior-chains} but for draws from the prior distribution.
    }\label{fig:surge-prior-chains}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{surge-test-statistics}
    \caption{
        Posterior predictive checks for the stationary \gls{gev} storm surge model (\cref{sec:storm-surge}).
        Each panel shows a different test statistic: partial autocorrelation at lags 1 and 2; sample maximum; sample minimum; sample median; and Mann-Kendall trend test statistic.
        The histograms show the distribution of each test statistic from the posterior predictive distribution.
        Orange lines show the test statistic's value in the observed data.
        Observed values near the mode of the posterior predictive distribution are consistent with, but do not guarantee, a good fit.
        For further discussion of posterior predictive checks, see Chapter 6 of \citet{Gelman:2014tc}.
    }\label{fig:surge-test-statistics}
\end{figure}

\begin{figure}
    \includegraphics[width=\textwidth]{scenario-map-height-slr}
    \caption{
        Expected total lifetime cost (damages plus up-front cost) as a function of \gls{slr} over the house lifetime and height increase $\Delta h$.
        Initial house elevation is fixed to \SI{1}{ft} below the \gls{bfe}.
    }\label{fig:scenario-map-height-slr}
\end{figure}

\section{Supplemental tables}

\begin{table}[h]
    \centering
    \caption{
        Diagnostic statistics for the \gls{mcmc} sampling for the storm surge posterior draws.
        Statistics include the mean and standard deviation of each parameter, the naive standard error and Monte Carlo standard error (which measure uncertainty in the mean), the effective sample size, $\hat{R}$ diagnostic, and effective samples per second, which describes sampling speed.
        In general, a $\hat{R}$ value close to one is consistent with, though does not guarantee, convergence.
    }\label{tab:surge-posterior-mcmc-diagnostics}
    \input{../../plots/surge-posterior-mcmc-diagnostics.tex}
\end{table}

\begin{table}[h]
    \centering
    \caption{As \cref{tab:surge-posterior-mcmc-diagnostics} but for draws from the prior distribution.}\label{tab:surge-prior-mcmc-diagnostics}
    \input{../../plots/surge-prior-mcmc-diagnostics.tex}
\end{table}

\end{document}
